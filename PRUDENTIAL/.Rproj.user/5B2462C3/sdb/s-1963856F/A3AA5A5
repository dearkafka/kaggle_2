{
    "contents" : "\n#set.seed(01*22*2016)\n\n# read in the data file--------------------------------------------------------------------------------\n\nrequire(data.table); require(xgboost); require(h2o); require(caret)\n\n\ntrain_raw <- fread(\"D:\\\\kaggle\\\\PRUDENTIAL\\\\Data\\\\train.csv\", data.table = F)\n\ntest  <- fread(\"D:\\\\kaggle\\\\PRUDENTIAL\\\\Data\\\\test.csv\", data.table = F)\n\ntrain_id <- train_raw$Id\n\ntrain$Id <- NULL;\n\nid <- test$Id; test$Id <- NULL\n\nresponse <- train$Response; train$Response <- NULL\n\n\ntmp <- rbind(train, test)\n\n\nrow_NA <- apply(tmp, 1, function(x) sum(is.na(x)))\n\ntmp$row_NA <- row_NA\n\n\n# dummify varible------------------------------------------------------------------------------\n\ntmp_dummy <- data.frame(tmp[,\"Product_Info_2\"])\n\ntmp_dummy[ , 1] <- as.factor(tmp_dummy[ , 1])\n\n\ndummies <- dummyVars( ~ ., data = tmp_dummy)\n\ngc()\n\ntmp_dummy <- predict(dummies, newdata = tmp_dummy)\n\ntmp_dummy <- data.frame(tmp_dummy)\n\ndim(tmp_dummy)\n\n\n# count the number of keywords row wise--------------------------------------------------------\n\nkeywords <- paste(\"Medical_Keyword_\", 1:48, sep=\"\")\n\ntmp_count <- tmp[, keywords]\n\n\ncount <- apply(tmp_count, 1, function(x) sum(x))\n\ntmp$count <- count\n\n\n##############################################################################################\n\ntmp[is.na(tmp)] <- -1\n\n\n# interaction features-------------------------------------------------------------------------\n\n\ntmp_int <- tmp[ , c(\"Ins_Age\", \"BMI\")]\n\nfor (i in 1:ncol(tmp_int)) {\n  \n  for (j in (i + 1) : (ncol(tmp_int) + 1)) {\n    \n    #    a = i; b= j\n    \n    var.x <- colnames(tmp_int)[i]\n    \n    var.y <- colnames(tmp_int)[j]\n    \n    var.new <- paste0(var.x, '_plus_', var.y)\n    \n    tmp_int[ , paste0(var.new)] <- tmp_int[, i] + tmp_int[, j]\n    \n  }\n}\n\n\ngc()\n\n\ntmp$Medical_History_10 <- NULL\n\ntmp$Medical_History_24 <- NULL\n\n\n############################################################################################################\n\n\n\nfeature.names <- names(tmp)\n\nfor (f in feature.names) {\n  \n  if (class(tmp[[f]])==\"character\") {\n    \n    levels <- unique(c(tmp[[f]]))\n    \n    tmp[[f]] <- as.integer(factor(tmp[[f]], levels=levels))\n    \n  }\n}\n\n\ntmp_new <- cbind(tmp, tmp_dummy, tmp_int)\n\ntmp_new <- tmp_new[ , !(names(tmp_new) %in% c(\"Product_Info_2\"))]\n\ntrain <- tmp_new[c(1:59381),]\n\ntest <- tmp_new[c(59382:79146),]\n\n\nrm(list = c(\"tmp\", \"tmp_category\", \"tmp_dummy\", \"tmp_high_card\", \"train_high_card\",\n            \n            \"test_high_card\", \"tmp_new\", \"tmp_count\"))\n\ngc()\n\n\n# begin blend-------------------------------------------------------------------------------------------\n\n### Returns train inidices for n_folds using StratifiedKFold\n\nrequire(caret)\n\n\nskf = createFolds(response, k = 5)\n\n\n### Create a list of models to run\n\nclfs <- c(\"rf\", \"gbm\", \"xgboost_1\",\"glm\")\n\n\n### Pre-allocate the data\n\n### For each model, add a column with N rows for each model\n\n\ndataset_blend_train = matrix(0, nrow(train), length(clfs))\n\ndataset_blend_test  = matrix(0, nrow(test), length(clfs))\n\n\n\n### Loop over the models------------------------------------------------------------------------------\n\nj <- 0 \n\nfor (clf in clfs)\n  \n{\n  \n  j <- j + 1\n  \n  print(paste(j,clf))\n  \n  \n  ### Create a tempory array that is (Holdout_Size, N_Folds).\n  \n  ### Number of testing data x Number of folds , we will take the mean of the predictions later\n  \n  dataset_blend_test_j = matrix(0, nrow(test), length(skf))\n  \n  print(paste(nrow(dataset_blend_test_j),ncol(dataset_blend_test_j)))\n  \n  \n  ### Loop over the folds\n  \n  i <- 0\n  \n  for (sk in skf) {\n    \n    i <- i + 1\n    \n    print(paste(\"Fold\", i))\n    \n    \n    ### Extract and fit the train/test section for each fold\n    \n    tmp_train <- unlist(skf[i])\n    \n    x_train = train[-tmp_train,]\n    \n    y_train = response[-tmp_train]\n    \n    x_test  = train[tmp_train,]\n    \n    y_test  = response[tmp_train] # write a scoring method to score auc from y_test\n    \n    \n    ### fit xgboost--------------------------------------------------------------------------------------\n    \n    if(clf == \"xgboost_1\"){\n      \n      \n      \n      feature.names <- names(train)\n      \n      # h<-sample(nrow(train),2000)\n      \n      # dval<-xgb.DMatrix(data=data.matrix(train[h,]),label=response[h])\n      \n      # dtrain<-xgb.DMatrix(data=data.matrix(train[-h,]),label=response[-h])\n      \n      dtrain<-xgb.DMatrix(data=data.matrix(x_train),label=y_train, missing = NaN)\n      \n      # watchlist<-list(val=dval,train=dtrain)\n      \n      param <- list(  print.every.n       = 20,\n                      \n                      objective           = \"reg:linear\",\n                      \n                      depth = 21,\n                      \n                      min_child_weight = 3,\n                      \n                      subsample = 0.71,\n                      \n                      eta = 0.01,\n                      \n                      silent = 0\n                      \n      )\n      \n      \n      \n      start <- Sys.time()\n      \n      \n      mod <- xgb.train(   params              = param,\n                          \n                          booster = \"gbtree\",\n                          \n                          data                = dtrain,\n                          \n                          nrounds             = 3000,\n                          \n                          verbose             = 1,\n                          \n                          maximize            = F\n                          \n      )\n      \n      dataset_blend_train[tmp_train, j] <- predict(mod, data.matrix(x_test), missing = NaN)\n      \n    }\n    \n    \n    # fit rf---------------------------------------------------------------------------------------------\n    \n    else if (clf == \"rf\"){\n      \n      # combining them in a df for maintaining format\n      \n      require(h2o)\n      \n      localH2O <- h2o.init(nthreads = -1, max_mem_size = '12g')\n      \n      \n      x_train$target <- (y_train)\n      \n      train.hex <- as.h2o(localH2O, object = x_train)\n      \n      test.hex <- as.h2o(localH2O, object = x_test)\n      \n      \n      myX <- names(train)\n      \n      myY <- \"target\"\n      \n      \n      test_rf <- h2o.randomForest(x = myX,\n                                  \n                                  y = myY,\n                                  \n                                  training_frame = train.hex,\n                                  \n                                  model_id = \"rf_1152016\", \n                                  \n                                  ntrees = 1500, \n                                  \n                                  max_depth = 10, \n                                  \n                                  binomial_double_trees = T, \n                                  \n                                  balance_classes = T, \n                                  \n                                  seed = 01*22*2016\n                                  \n      )\n      \n      pred_rf <- h2o.predict(object = test_rf, newdata = test.hex)\n      \n      pred_rf <- as.data.frame(pred_rf)\n      \n      \n      dataset_blend_train[tmp_train, j] <- pred_rf$predict\n      \n      \n      \n    }\n    \n    \n    \n    # fit gbm--------------------------------------------------------------------------------------------\n    \n    else if(clf == \"gbm\"){\n      \n      require(h2o)\n      \n      localH2O <- h2o.init(nthreads = -1, max_mem_size = '12g')\n      \n      \n      \n      x_train$target <- as.factor(y_train)\n      \n      train.hex <- as.h2o(localH2O, object = x_train)\n      \n      test.hex <- as.h2o(localH2O, object = x_test)\n      \n      \n      myX <- names(train)\n      \n      myY <- \"target\"\n      \n      \n      \n      test_gbm <- h2o.gbm(x = myX,\n                          \n                          y = myY,\n                          \n                          training_frame = train.hex,\n                          \n                          model_id = \"gbm_01152016\", \n                          \n                          ntrees =  2000, \n                          \n                          max_depth = 20, \n                          \n                          learn_rate = 0.014, \n                          \n                          seed = 01*22*2016, \n                          \n                          distribution= \"multinomial\", \n                          \n                          min_rows = 9\n                          \n      )\n      \n      \n      pred_gbm <- h2o.predict(object = test_gbm, newdata = test.hex)\n      \n      pred_gbm <- as.data.frame(pred_gbm)\n      \n      dataset_blend_train[tmp_train, j] <- pred_gbm$predict\n      \n      \n      \n    }\n    \n    \n    \n    # fit glm--------------------------------------------------------------------------------------------\n    \n    else if(clf == \"glm\"){\n      \n      require(h2o)\n      \n      localH2O <- h2o.init(nthreads = -1)\n      \n      \n      \n      \n      x_train$target <- y_train\n      \n      train.hex <- as.h2o(localH2O, object = x_train)\n      \n      test.hex <- as.h2o(localH2O, object = x_test)\n      \n      \n      myX <- names(train)\n      \n      myY <- \"target\"\n      \n      \n      test_glm <- h2o.glm( x = myX,\n                           \n                           y = myY,\n                           \n                           training_frame = train.hex,\n                           \n                           family = \"poisson\",\n                           \n                           lambda_search = TRUE,\n                           \n                           nlambdas = 10, \n                           \n                           model_id = \"glm_test\", \n                           \n                           solver = \"L_BFGS\",\n                           \n                           #keep_cross_validation_predictions = T,\n                           \n                           alpha = c(0, 0.1, 0.2, 0.3, 0.4, 0.6, 0.8, 1), \n                           \n                           link = \"log\", \n                           \n                           standardize = T\n                           \n      )\n      \n      pred_glm <- h2o.predict(object = test_glm, newdata = test.hex)\n      \n      pred_glm <- as.data.frame(pred_glm)\n      \n      \n      dataset_blend_train[tmp_train, j] <- pred_glm$predict\n      \n    }\n    \n    \n    \n    \n    \n    # predict xgboost for test set---------------------------------------------------------------------\n    \n    if(clf == \"xgboost_1\")\n      \n    {\n      \n      print(paste(\"predicting xgboost for test set ; Fold :\", i))\n      \n      dataset_blend_test_j[, i] <- predict(mod, data.matrix(test), missing = NaN)\n      \n    }\n    \n    # predict rf for test set------------------------------------------------------------------\n    \n    \n    else if(clf == \"rf\"){\n      \n      print(paste(\"predicting rf for test set ; Fold :\", i))\n      \n      test.hex <- as.h2o(localH2O, object = test)\n      \n      pred_rf <- h2o.predict(object = test_rf, newdata = test.hex)\n      \n      pred_rf <- as.data.frame(pred_rf)\n      \n      \n      dataset_blend_test_j[, i] <- pred_rf$predict\n      \n    }\n    \n    # predict gbm for test set---------------------------------------------------------------\n    \n    \n    else if(clf == \"gbm\"){\n      \n      print(paste(\"predicting gbm for test set ; Fold :\", i))\n      \n      test.hex <- as.h2o(localH2O, object = test)\n      \n      pred_gbm <- h2o.predict(object = test_gbm, newdata = test.hex)\n      \n      pred_gbm <- as.data.frame(pred_gbm)\n      \n      dataset_blend_test_j[, i] <- pred_gbm$predict\n    }\n    \n    \n    # predict glm for test set------------------------------------------------------------------\n    \n    else if(clf == \"glm\"){\n      \n      print(paste(\"predicting glm for test set ; Fold :\", i))\n      \n      test.hex <- as.h2o(localH2O, object = test)\n      \n      pred_glm <- h2o.predict(object = test_glm, newdata = test.hex)\n      \n      pred_glm <- as.data.frame(pred_glm)\n      \n      \n      dataset_blend_test_j[, i] <- pred_glm$predict\n      \n    }\n    \n  }\n  \n  dataset_blend_test[,j] = rowMeans(dataset_blend_test_j)  \n  \n}\n\nrequire(readr)\n\n# save train set and oof test datasets for higher level training----------------------------------\n\n\nwrite_csv(as.data.frame(dataset_blend_test), \"D:\\\\kaggle\\\\PRUDENTIAL\\\\blend\\\\blend_test_01232016\")\n\nwrite_csv(as.data.frame(dataset_blend_train), \"D:\\\\kaggle\\\\PRUDENTIAL\\\\blend\\\\blend_train_01232016\")\n\n\n# check whether oof df's are saved before proceeding further\n\n\n# use dataset_blend_train to train a second level model-------------------------------------------\n\ntrain <- cbind(train, data.frame(dataset_blend_train))\n\ntest <- cbind(test, data.frame(dataset_blend_test))\n\n\n# train a second level using xgboost-------------------------------------------------------------\n\n## use crossval setup to get optimum cutpoints equation\n\nskf = createFolds(response, k = 5 )\n\ndataset_blend_train  = matrix(0, nrow(train), 1)\n\n### Loop over the folds\n\ni <- 0\n\nfor (sk in skf) {\n  \n  i <- i + 1\n  \n  print(paste(\"Fold\", i))\n  \n  \n  ### Extract and fit the train/test section for each fold\n  \n  tmp_train <- unlist(skf[i])\n  \n  x_train = train[-tmp_train, ]\n  \n  y_train = response[-tmp_train]\n  \n  x_test  = train[tmp_train,]\n  \n  y_test  = response[tmp_train]\n  \n  \n  dtrain <- xgb.DMatrix(data = data.matrix(x_train),label = y_train, missing = NaN )\n  \n  param <- list(  print.every.n       = 20,\n                  \n                  objective           = \"reg:linear\",\n                  \n                  depth = 21,\n                  \n                  min_child_weight = 3,\n                  \n                  subsample = 0.71,\n                  \n                  eta = 0.01,\n                  \n                  silent = 0\n                  \n  )\n  \n  \n  \n  start <- Sys.time()\n  \n  \n  mod <- xgb.train(   params              = param,\n                      \n                      booster = \"gbtree\",\n                      \n                      data                = dtrain,\n                      \n                      nrounds             = 3000,\n                      \n                      verbose             = 1,\n                      \n                      maximize            = F\n                      \n  )\n  \n  \n  dataset_blend_train[tmp_train, 1] <- predict(mod, data.matrix(x_test), missing = NaN)\n  \n}\n\n\n#############################################################################################################\n\n\nSQWKfun = function(x = seq(1.5, 7.5, by = 1), pred) {\n  preds = pred$predict\n  true = pred$Response\n  cuts = c(min(preds), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(preds))\n  preds = as.numeric(Hmisc::cut2(preds, cuts))\n  err = Metrics::ScoreQuadraticWeightedKappa(preds, true, 1, 8)\n  return(-err)\n}\n\n# optimise using optim on train and predict on test---------------------------------------------- \n\npred = data.frame(Id=train_id, Response=response, predict=dataset_blend_train)\n\n# on further iterations instead of using oof preds, actual train preds may be used | {might overfit}\n\noptCuts = optim(seq(1.5, 7.5, by = 1), SQWKfun, pred = pred)\n\nprint(optCuts)\n\n\n# predict using optimal cut points---------------------------------------------------------------\n\ndtrain <- xgb.DMatrix(data = data.matrix(train),label = response, missing = NaN )\n\nparam <- list(  print.every.n       = 20,\n                \n                objective           = \"reg:linear\",\n                \n                depth = 21,\n                \n                min_child_weight = 3,\n                \n                subsample = 0.71,\n                \n                eta = 0.01,\n                \n                silent = 0\n                \n)\n\n\n\nstart <- Sys.time()\n\n\nmod <- xgb.train(   params              = param,\n                    \n                    booster = \"gbtree\",\n                    \n                    data                = dtrain,\n                    \n                    nrounds             = 3000,\n                    \n                    verbose             = 1,\n                    \n                    maximize            = F\n                    \n)\n\npredict_test = predict(mod, data.matrix((test)), missing = NaN)\n\npreds = as.numeric(Hmisc::cut2(predict_test, c(-Inf, optCuts$par, Inf)))\n\n\n## create submission file-----------------------------------------------------------------------\n\nsubmission = data.frame(Id = id)\n\nsubmission$Response = as.integer(preds)\n\nwrite_csv(submission, \"D:\\\\kaggle\\\\PRUDENTIAL\\\\submission\\\\01232016.csv\")",
    "created" : 1453558332873.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2015413035",
    "id" : "A3AA5A5",
    "lastKnownWriteTime" : 1456072646,
    "path" : "D:/kaggle/PRUDENTIAL/version_control/0_67507.R",
    "project_path" : "version_control/0_67507.R",
    "properties" : {
        "tempName" : "Untitled3"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_source"
}