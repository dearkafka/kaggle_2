# rm(tmp_unique)
for(f in names(tmp_char)){
levels <- unique(tmp_char[, f])
tmp_char[,f] <- factor(tmp_char[,f], levels = levels)
}
dummies <- dummyVars( ~., data = tmp_char)
tmp_char <- predict(dummies, newdata = tmp_char)
tmp_char <- data.frame(tmp_char)
rm(dummies)
gc()
for (f in names(tmp)) {
if (class(tmp[[f]])=="character") {
levels <- unique(tmp[[f]])
tmp[[f]] <- as.integer(factor(tmp[[f]], levels=levels))
}
}
tmp_new <- cbind(tmp, tmp_char)
rm(test_raw); rm(train_raw); rm(tmp_char)
#############################################################################################
# add interaction terms
imp <- read_csv("D:\\kaggle\\HOMESITE\\FEATURE_IMP\\12062015_1.csv")
top_50 <- imp$Feature[1:5]
tmp_int <- tmp[, top_50]
for (f in top_50) {
if (class(tmp_int[[f]])=="character") {
levels <- unique(tmp_int[[f]])
tmp_int[[f]] <- as.integer(factor(tmp_int[[f]], levels=levels))
}
}
gc()
rm(imp);
#############################################################################################
# plus interaction
for (i in 1:ncol(tmp_int)) {
for (j in (i + 1) : (ncol(tmp_int) + 1)) {
#    a = i; b= j
var.x <- colnames(tmp_int)[i]
var.y <- colnames(tmp_int)[j]
var.new <- paste0(var.x, '_plus_', var.y)
tmp_int[ , paste0(var.new)] <- tmp_int[, i] + tmp_int[, j]
}
}
gc()
tmp_new <- cbind(tmp_new, tmp_int)
rm(tmp_int)
gc()
############################################################################################
# create - interaction features
# add interaction terms
imp <- read_csv("D:\\kaggle\\HOMESITE\\FEATURE_IMP\\12062015_1.csv")
top_50 <- imp$Feature[1:5]
tmp_int <- tmp[, top_50]
for (f in top_50) {
if (class(tmp_int[[f]])=="character") {
levels <- unique(tmp_int[[f]])
tmp_int[[f]] <- as.integer(factor(tmp_int[[f]], levels=levels))
}
}
gc()
rm(imp);
for (i in 1:ncol(tmp_int)) {
for (j in (i + 1) : (ncol(tmp_int) + 1)) {
var.x <- colnames(tmp_int)[i]
var.y <- colnames(tmp_int)[j]
var.new <- paste0(var.x, '_minus_', var.y)
tmp_int[ , paste0(var.new)] <- tmp_int[, i] - tmp_int[, j]
}
}
gc()
tmp_new <- cbind(tmp_new, tmp_int)
rm(tmp_int)
gc()
#############################################################################################
# create * interaction features
# add interaction terms
imp <- read_csv("D:\\kaggle\\HOMESITE\\FEATURE_IMP\\12062015_1.csv")
top_50 <- imp$Feature[1:5]
tmp_int <- tmp[, top_50]
for (f in top_50) {
if (class(tmp_int[[f]])=="character") {
levels <- unique(tmp_int[[f]])
tmp_int[[f]] <- as.integer(factor(tmp_int[[f]], levels=levels))
}
}
gc()
rm(imp);
for (i in 1:ncol(tmp_int)) {
for (j in (i + 1) : (ncol(tmp_int) + 1)) {
var.x <- colnames(tmp_int)[i]
var.y <- colnames(tmp_int)[j]
var.new <- paste0(var.x, '_mult_', var.y)
tmp_int[ , paste0(var.new)] <- tmp_int[, i] * tmp_int[, j]
}
}
tmp_new <- cbind(tmp_new, tmp_int)
rm(tmp_int)
gc()
tmp_new <- tmp_new[, !(names(tmp_new) %in% top_50)]
imp <- read_csv("D:\\kaggle\\HOMESITE\\FEATURE_IMP\\12062015_1.csv")
top_50 <- imp$Feature[1:5]
tmp_int <- tmp[, top_50]
for (f in top_50) {
if (class(tmp_int[[f]])=="character") {
levels <- unique(tmp_int[[f]])
tmp_int[[f]] <- as.integer(factor(tmp_int[[f]], levels=levels))
}
}
tmp_new <- cbind(tmp_new, tmp_int)
rm(tmp_int); rm(tmp)
rm(tmp); rm(test_raw); rm(train_raw); rm(tmp_char); rm(tmp_int); rm(imp)
train <- tmp_new[c(1:260753), ]
test <- tmp_new[c(260754:434589), ]
rm(tmp_new)
gc()
# create folds------------------------------------------------------------------------------------------
dataset_blend_train = matrix(0, nrow(train), 1)
dataset_blend_test_j = matrix(0, nrow(test), 2) # this should always be number of folds
dataset_blend_test = matrix(0, nrow(test), 1)
# start iteration loop---------------------------------------------------------------------------------
for(j in 1:2)
j = 1
print(paste("starting glm iteration ; number :", j))
set.seed(2*05*2016*j)
require(caret)
skf = createFolds(response, k = 2)
print(paste(nrow(dataset_blend_test_j),ncol(dataset_blend_test_j)))
# start fold loop------------------------------------------------------------------------------------
### Loop over the folds
i <- 0
for (sk in skf) {
i <- i + 1
print(paste("Fold", i))
### Extract and fit the train/test section for each fold
i = 2
tmp_train <- unlist(skf[i])
x_train = train[-tmp_train,]
y_train = response[-tmp_train]
x_test  = train[tmp_train,]
y_test  = response[tmp_train]
require(h2o)
localH2O <- h2o.init(nthreads = 4, max_mem_size = '12g')
x_train$target <- (y_train)
train.hex <- as.h2o(localH2O, object = x_train)
test.hex <- as.h2o(localH2O, object = x_test)
myX <- names(train)
myY <- "target"
print(paste("training glm iteration :", j, "for Fold ; number :", i))
test_glm <- h2o.glm( x = myX,
y = myY,
training_frame = train.hex,
family = "binomial",
solver = "L_BFGS",
link = "logit",
standardize = T,
lambda_search = T
)
pred_glm <- h2o.predict(test_glm, newdata = test.hex)
pred_glm <- as.data.frame(pred_glm)
#     test_glm <- glm(formula = target ~ ., family = binomial(link = "logit"), data = x_train)
dataset_blend_train[tmp_train, j] <- pred_glm$p1
print(paste("predicting glm for test set iteration :", j, "; Fold :", i))
test.hex <- as.h2o(localH2O, object = test)
pred_glm <- h2o.predict(object = test_glm, newdata = test.hex)
pred_glm <- as.data.frame(pred_glm)
dataset_blend_test_j[, i] <- pred_glm$p1
}
# create folds------------------------------------------------------------------------------------------
dataset_blend_train = matrix(0, nrow(train), 1)
dataset_blend_test_j = matrix(0, nrow(test), 2) # this should always be number of folds
dataset_blend_test = matrix(0, nrow(test), 1)
# start iteration loop---------------------------------------------------------------------------------
for(j in 1:2)
j = 1
print(paste("starting glm iteration ; number :", j))
set.seed(2*05*2016*j)
require(caret)
skf = createFolds(response, k = 2)
print(paste(nrow(dataset_blend_test_j),ncol(dataset_blend_test_j)))
# start fold loop------------------------------------------------------------------------------------
### Loop over the folds
i <- 0
for (sk in skf) {
i <- i + 1
print(paste("Fold", i))
### Extract and fit the train/test section for each fold
tmp_train <- unlist(skf[i])
x_train = train[-tmp_train,]
y_train = response[-tmp_train]
x_test  = train[tmp_train,]
y_test  = response[tmp_train]
require(h2o)
localH2O <- h2o.init(nthreads = 4, max_mem_size = '12g')
x_train$target <- (y_train)
train.hex <- as.h2o(localH2O, object = x_train)
test.hex <- as.h2o(localH2O, object = x_test)
myX <- names(train)
myY <- "target"
print(paste("training glm iteration :", j, "for Fold ; number :", i))
test_glm <- h2o.glm( x = myX,
y = myY,
training_frame = train.hex,
family = "binomial",
solver = "L_BFGS",
link = "logit",
standardize = T,
lambda_search = T
)
pred_glm <- h2o.predict(test_glm, newdata = test.hex)
pred_glm <- as.data.frame(pred_glm)
#     test_glm <- glm(formula = target ~ ., family = binomial(link = "logit"), data = x_train)
dataset_blend_train[tmp_train, j] <- pred_glm$p1
print(paste("predicting glm for test set iteration :", j, "; Fold :", i))
test.hex <- as.h2o(localH2O, object = test)
pred_glm <- h2o.predict(object = test_glm, newdata = test.hex)
pred_glm <- as.data.frame(pred_glm)
dataset_blend_test_j[, i] <- pred_glm$p1
}
dataset_blend_train = matrix(0, nrow(train), 1)
dataset_blend_test_j = matrix(0, nrow(test), 2) # this should always be number of folds
dataset_blend_test = matrix(0, nrow(test), 1)
j = 1
print(paste("starting rf iteration ; number :", j))
set.seed(2*05*2016*j)
require(caret)
skf = createFolds(response, k = 2)
print(paste(nrow(dataset_blend_test_j),ncol(dataset_blend_test_j)))
# start fold loop------------------------------------------------------------------------------------
### Loop over the folds
i <- 0
for (sk in skf) {
i <- i + 1
print(paste("Fold", i))
### Extract and fit the train/test section for each fold
tmp_train <- unlist(skf[i])
x_train = train[-tmp_train,]
y_train = response[-tmp_train]
x_test  = train[tmp_train,]
y_test  = response[tmp_train]
require(h2o)
localH2O <- h2o.init(nthreads = -1, max_mem_size = '12g', assertion = F)
x_train$target <- as.factor(y_train)
train.hex <- as.h2o(x_train)
test.hex <- as.h2o(x_test)
myX <- names(train)
myY <- "target"
require(h2o)
localH2O <- h2o.init(nthreads = -1)
x_train$target <- (y_train)
train.hex <- as.h2o(x_train)
test.hex <- as.h2o(x_test)
myX <- names(train)
myY <- "target"
print(paste("training rf for iteration : ", j, "Fold ; number :", i))
test_rf <- h2o.randomForest(x = myX,
y = myY,
training_frame = train.hex,
ntrees = 1000,
max_depth = 10,
binomial_double_trees = T,
balance_classes = T
)
pred_rf <- h2o.predict(object = test_rf, newdata = test.hex)
pred_rf <- as.data.frame(pred_rf)
dataset_blend_train[tmp_train, j] <- pred_rf$predict
print(paste("predicting rf for test set iteration :", j, "; Fold :", i))
test.hex <- as.h2o(test)
pred_rf <- h2o.predict(object = test_rf, newdata = test.hex)
pred_rf <- as.data.frame(pred_rf)
dataset_blend_test_j[, i] <- pred_rf$predict
}
dataset_blend_test[, j] <- rowMeans(dataset_blend_test_j)
require(readr)
write_csv(data.frame(dataset_blend_train), "D:\\kaggle\\HOMESITE\\blend\\bag\\rf\\blend_train_rf_02062016.csv")
write_csv(data.frame(dataset_blend_test), "D:\\kaggle\\HOMESITE\\blend\\bag\\rf\\blend_test_rf_02062016.csv")
# create folds------------------------------------------------------------------------------------------
dataset_blend_train = matrix(0, nrow(train), 1)
dataset_blend_test_j = matrix(0, nrow(test), 2) # this should always be number of folds
dataset_blend_test = matrix(0, nrow(test), 1)
# start iteration loop---------------------------------------------------------------------------------
for(j in 1:2)
j = 1
print(paste("starting glm iteration ; number :", j))
set.seed(2*05*2016*j)
require(caret)
skf = createFolds(response, k = 2)
print(paste(nrow(dataset_blend_test_j),ncol(dataset_blend_test_j)))
# start fold loop------------------------------------------------------------------------------------
### Loop over the folds
i <- 0
for (sk in skf) {
i <- i + 1
print(paste("Fold", i))
### Extract and fit the train/test section for each fold
tmp_train <- unlist(skf[i])
x_train = train[-tmp_train,]
y_train = response[-tmp_train]
x_test  = train[tmp_train,]
y_test  = response[tmp_train]
require(h2o)
localH2O <- h2o.init(nthreads = 4, max_mem_size = '12g')
x_train$target <- (y_train)
train.hex <- as.h2o(localH2O, object = x_train)
test.hex <- as.h2o(localH2O, object = x_test)
myX <- names(train)
myY <- "target"
print(paste("training glm iteration :", j, "for Fold ; number :", i))
test_glm <- h2o.glm( x = myX,
y = myY,
training_frame = train.hex,
family = "binomial",
solver = "L_BFGS",
link = "logit",
standardize = T,
lambda_search = T
)
pred_glm <- h2o.predict(test_glm, newdata = test.hex)
pred_glm <- as.data.frame(pred_glm)
#     test_glm <- glm(formula = target ~ ., family = binomial(link = "logit"), data = x_train)
dataset_blend_train[tmp_train, j] <- pred_glm$p1
print(paste("predicting glm for test set iteration :", j, "; Fold :", i))
test.hex <- as.h2o(localH2O, object = test)
pred_glm <- h2o.predict(object = test_glm, newdata = test.hex)
pred_glm <- as.data.frame(pred_glm)
dataset_blend_test_j[, i] <- pred_glm$p1
}
dataset_blend_test[, j] <- rowMeans(dataset_blend_test_j)
require(readr)
write_csv(data.frame(dataset_blend_train), "D:\\kaggle\\HOMESITE\\blend\\bag\\glm\\blend_train_glm_02062016.csv")
write_csv(data.frame(dataset_blend_test), "D:\\kaggle\\HOMESITE\\blend\\bag\\glm\\blend_test_glm_02062016.csv")
#############################################################################################
require(doParallel); require(xgboost); require(caret)
cl <- makeCluster(4); registerDoParallel(cl)
# create folds------------------------------------------------------------------------------------------
dataset_blend_train = matrix(0, nrow(train), 1)
dataset_blend_test_j = matrix(0, nrow(test), 2) # this should always be number of folds
dataset_blend_test = matrix(0, nrow(test), 1)
# start iteration loop---------------------------------------------------------------------------------
j = 1
print(paste("starting xgboost iteration ; number :", j))
set.seed(2*05*2016*j)
require(caret)
skf = createFolds(response, k = 2)
print(paste(nrow(dataset_blend_test_j),ncol(dataset_blend_test_j)))
# start fold loop------------------------------------------------------------------------------------
### Loop over the folds
i <- 0
for (sk in skf) {
i <- i + 1
print(paste("Fold", i))
### Extract and fit the train/test section for each fold
tmp_train <- unlist(skf[i])
x_train = train[-tmp_train,]
y_train = response[-tmp_train]
x_test  = train[tmp_train,]
y_test  = response[tmp_train]
feature.names <- names(train)
dtrain<-xgb.DMatrix(data=data.matrix(x_train),label=y_train, missing = NaN)
param <- list(  objective           = "binary:logistic",
booster = "gbtree",
eval_metric = "auc",
eta                 = 0.023, # 0.06, #0.01,
max_depth           = 8, #changed from default of 8
subsample           = 0.83, # 0.7
colsample_bytree    = 0.77, # 0.7
num_parallel_tree = 2
)
# start training------------------------------------------------------------------------------
print(paste("training xgboost for iteration :", j, "Fold ; number :", i))
mod <- xgb.train(   params              = param,
data                = dtrain,
nrounds             = 3000,
verbose             = 1,
maximize            = T,
nthread = 4)
dataset_blend_train[tmp_train, j] <- predict(mod, data.matrix(x_test), missing = NaN)
print(paste("predicting xgboost for test set iteration:", j, " ; Fold :", i))
dataset_blend_test_j[, i] <- predict(mod, data.matrix(test), missing = NaN)
}
dataset_blend_test[, j] <- rowMeans(dataset_blend_test_j)
require(readr)
write_csv(data.frame(dataset_blend_train), "D:\\kaggle\\HOMESITE\\blend\\bag\\xgb\\blend_train_xgb_02052016.csv")
write_csv(data.frame(dataset_blend_test), "D:\\kaggle\\HOMESITE\\blend\\bag\\xgb\\blend_test_xgb_02052016.csv")
dataset_blend_train <- read_csv("D:\\kaggle\\HOMESITE\\blend\\bag\\FINAL\\blend_train_02052016.csv")
dataset_blend_test <- read_csv("D:\\kaggle\\HOMESITE\\blend\\bag\\FINAL\\blend_test_02052016.csv")
train <- cbind(train, data.frame(dataset_blend_train))
test <- cbind(test, data.frame(dataset_blend_test))
feature.names <- names(train)
dtrain<-xgb.DMatrix(data=data.matrix(train),label=response, missing = NaN)
param <- list(  objective           = "binary:logistic",
booster = "gbtree",
eval_metric = "auc",
eta                 = 0.023, # 0.06, #0.01,
max_depth           = 6, #changed from default of 8
subsample           = 0.83, # 0.7
colsample_bytree    = 0.77, # 0.7
num_parallel_tree = 2
)
mod <- xgb.train(   params              = param,
data                = dtrain,
nrounds             = 3000,
verbose             = 1,
maximize            = T,
nthread = 4)
pred <- predict(mod, data.matrix(test), missing = NaN)
## create submission file-----------------------------------------------------------------------
submission <- data.frame(QuoteNumber = id, QuoteConversion_Flag = pred)
write_csv(submission, "D:\\kaggle\\HOMESITE\\submission\\02072016_1_xgb.csv")
dataset_blend_train <- read_csv("D:\\kaggle\\HOMESITE\\blend\\bag\\FINAL\\blend_train_02052016.csv")
dataset_blend_test <- read_csv("D:\\kaggle\\HOMESITE\\blend\\bag\\FINAL\\blend_test_02052016.csv")
dataset_blend_train <- read_csv("D:\\kaggle\\HOMESITE\\blend\\bag\\FINAL\\blend_train_02052016.csv")
dataset_blend_test <- read_csv("D:\\kaggle\\HOMESITE\\blend\\bag\\FINAL\\blend_test_02052016.csv")
x <- as.matrix(dataset_blend_train)
y <- as.numeric(response)
require(deepnet)
nn <- dbn.dnn.train(x,y,hidden = c(1),
activationfun = "sigm",learningrate = 0.2,momentum = 0.8)
nn_predict <- n.predict(nn,x)
nn_predict <- nn.predict(nn,x)
nn_predict_test <- nn.predict(nn,dataset_blend_test)
pred <- nn.predict(nn,dataset_blend_test)
submission <- data.frame(QuoteNumber = id, QuoteConversion_Flag = pred)
write_csv(submission, "D:\\kaggle\\HOMESITE\\submission\\02082016.csv")
rm(list = ls())
require(readr)
# find a better way to unzip files over here
input <- list.files(path = "D:\\kaggle\\HOMESITE\\ensemble\\csv")
input
for(i in 1:length(input))
{
assign(paste0("df_", i),
read_csv(paste0("D:\\kaggle\\HOMESITE\\ensemble\\csv", input[i])))
}
require(readr)
# find a better way to unzip files over here
input <- list.files(path = "D:\\kaggle\\HOMESITE\\ensemble\\csv")
for(i in 1:length(input))
{
assign(paste0("df_", i),
read_csv(paste0("D:\\kaggle\\HOMESITE\\ensemble\\csv", input[i])))
}
input[1]
require(readr)
# find a better way to unzip files over here
input <- list.files(path = "D:\\kaggle\\HOMESITE\\ensemble\\csv")
for(i in 1:length(input))
{
assign(paste0("df_", i),
read_csv(paste0("D:\\kaggle\\HOMESITE\\ensemble\\csv\\", input[i])))
}
tmp <- cbind.data.frame(df_1$QuoteConversion_Flag,
df_2$QuoteConversion_Flag,
df_3$QuoteConversion_Flag,
df_4$QuoteConversion_Flag,
df_5$QuoteConversion_Flag,
df_6$QuoteConversion_Flag
)
QuoteConversion_Flag <- (rowMeans(tmp))
tmp <- cbind.data.frame(df_1$QuoteConversion_Flag,
df_2$QuoteConversion_Flag,
df_3$QuoteConversion_Flag,
df_4$QuoteConversion_Flag,
df_5$QuoteConversion_Flag
)
QuoteConversion_Flag <- (rowMeans(tmp))
df <- data.frame(QuoteNumber = df_1$QuoteNumber, QuoteConversion_Flag = QuoteConversion_Flag)
names(df)
write_csv(df, "df_average_all_02072016.csv")
require(readr)
# find a better way to unzip files over here
input <- list.files(path = "D:\\kaggle\\HOMESITE\\ensemble\\csv")
for(i in 1:length(input))
{
assign(paste0("df_", i),
read_csv(paste0("D:\\kaggle\\HOMESITE\\ensemble\\csv\\", input[i])))
}
rm(list = ls())
require(readr)
# find a better way to unzip files over here
input <- list.files(path = "D:\\kaggle\\HOMESITE\\ensemble\\csv")
for(i in 1:length(input))
{
assign(paste0("df_", i),
read_csv(paste0("D:\\kaggle\\HOMESITE\\ensemble\\csv\\", input[i])))
}
tmp <- cbind.data.frame(df_1$QuoteConversion_Flag,
df_2$QuoteConversion_Flag,
df_3$QuoteConversion_Flag,
df_4$QuoteConversion_Flag
)
QuoteConversion_Flag <- (rowMeans(tmp))
df <- data.frame(QuoteNumber = df_1$QuoteNumber, QuoteConversion_Flag = QuoteConversion_Flag)
names(df)
write_csv(df, "df_average_all_02072016_1.csv")
# now for the stacking part
require(readr)
# find a better way to unzip files over here
input <- list.files(path = "D:\\kaggle\\HOMESITE\\ensemble\\csv")
for(i in 1:length(input))
{
assign(paste0("df_", i),
read_csv(paste0("D:\\kaggle\\HOMESITE\\ensemble\\csv\\", input[i])))
}
tmp <- cbind.data.frame(df_1$QuoteConversion_Flag,
df_2$QuoteConversion_Flag,
df_3$QuoteConversion_Flag
)
QuoteConversion_Flag <- (rowMeans(tmp))
df <- data.frame(QuoteNumber = df_1$QuoteNumber, QuoteConversion_Flag = QuoteConversion_Flag)
names(df)
write_csv(df, "df_average_all_02072016_2.csv")
# now for the stacking part
rm(list = ls())
