eta = 0.01,
silent = 0,
colsample_bytree = 0.8
)
start <- Sys.time()
mod <- xgb.train(   params              = param,
booster = "gbtree",
data                = dtrain,
nrounds             = 4000,
verbose             = 1,
maximize            = F
)
predict = predict(mod, data.matrix((train)))
predict_test = predict(mod, data.matrix((test)))
SQWKfun = function(x = seq(1.5, 7.5, by = 1), pred) {
preds = pred$predict
true = pred$Response
cuts = c(min(preds), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(preds))
preds = as.numeric(Hmisc::cut2(preds, cuts))
err = Metrics::ScoreQuadraticWeightedKappa(preds, true, 1, 8)
return(-err)
}
pred = data.frame(Id=train_id, Response=response, predict=predict)
optCuts = optim(seq(1.5, 7.5, by = 1), SQWKfun, pred = pred)
print(optCuts)
preds = as.numeric(Hmisc::cut2(predict_test, c(-Inf, optCuts$par, Inf)))
submission <- data.frame(Id=id, Response=preds)
require(readr)
write_csv(submission, "01102016_3.csv")
feature_imp <- xgb.importance( model = mod)
feature_imp$Feature <- as.numeric(feature_imp$Feature)
names <- c()
for(i in 1:length(feature_imp$Feature)){
names <- c(names(train)[feature_imp$Feature[i]], names)
}
feature_imp$names <- names
write_csv(feature_imp, "feature_imp_raw.csv")
i = 1
skf = createFolds(response, k = 5 , list = TRUE, returnTrain = TRUE)
tmp_train <- unlist(skf[i])
x_train = train[-tmp_train, ]
y_train = response[-tmp_train]
x_test  = train[tmp_train,]
y_test  = response[tmp_train]
dtrain <- xgb.DMatrix(data=data.matrix(x_train),label=y_train)
param <- list(  print.every.n       = 20,
objective           = "reg:linear",
depth = 21,
min_child_weight = 3,
subsample = 0.71,
eta = 0.01,
silent = 0,
colsample_bytree = 0.8
)
start <- Sys.time()
mod <- xgb.train(   params              = param,
booster = "gbtree",
data                = dtrain,
nrounds             = 40,
verbose             = 1,
maximize            = F
)
dataset_blend_train[tmp_train, 1] <- predict(mod, data.matrix(x_test))
dataset_blend_train  = matrix(0, nrow(test), 1)
dataset_blend_train  = matrix(0, nrow(train), 1)
dataset_blend_train[tmp_train, 1] <- predict(mod, data.matrix(x_test))
set.seed(01112016)
require(data.table); require(xgboost)
rm(list = ls())
train <- fread("D:\\kaggle\\PRUDENTIAL\\Data\\train.csv", data.table = F)
test  <- fread("D:\\kaggle\\PRUDENTIAL\\Data\\test.csv", data.table = F)
train_id <- train$Id
train$Id <- NULL;
id <- test$Id; test$Id <- NULL
response <- train$Response; train$Response <- NULL
tmp <- rbind(train, test)
tmp[is.na(tmp)] <- -1
# row_NA <- apply(tmp, 1, function(x) sum(x == -1))
# tmp$row_NA <- row_NA
feature.names <- names(tmp)
for (f in feature.names) {
if (class(tmp[[f]])=="character") {
levels <- unique(c(tmp[[f]]))
tmp[[f]] <- as.integer(factor(tmp[[f]], levels=levels))
}
}
train <- tmp[c(1:59381),]
test <- tmp[c(59382:79146),]
gc()
skf = createFolds(response, k = 5 , list = TRUE, returnTrain = TRUE)
dataset_blend_train  = matrix(0, nrow(train), 1)
### Loop over the folds
i <- 0
for (sk in skf) {
i <- i + 1
print(paste("Fold", i))
### Extract and fit the train/test section for each fold
tmp_train <- unlist(skf[i])
x_train = train[-tmp_train, ]
y_train = response[-tmp_train]
x_test  = train[tmp_train,]
y_test  = response[tmp_train]
dtrain <- xgb.DMatrix(data=data.matrix(x_train),label=y_train)
param <- list(  print.every.n       = 20,
objective           = "reg:linear",
depth = 21,
min_child_weight = 3,
subsample = 0.71,
eta = 0.01,
silent = 0,
colsample_bytree = 0.8
)
start <- Sys.time()
mod <- xgb.train(   params              = param,
booster = "gbtree",
data                = dtrain,
nrounds             = 4000,
verbose             = 1,
maximize            = F
)
dataset_blend_train[tmp_train, 1] <- predict(mod, data.matrix(x_test))
}
predict_test = predict(mod, data.matrix((test)))
SQWKfun = function(x = seq(1.5, 7.5, by = 1), pred) {
preds = pred$predict
true = pred$Response
cuts = c(min(preds), x[1], x[2], x[3], x[4], x[5], x[6], x[7], max(preds))
preds = as.numeric(Hmisc::cut2(preds, cuts))
err = Metrics::ScoreQuadraticWeightedKappa(preds, true, 1, 8)
return(-err)
}
pred = data.frame(Id=train_id, Response=response, predict=dataset_blend_train)
optCuts = optim(seq(1.5, 7.5, by = 1), SQWKfun, pred = pred)
print(optCuts)
preds = as.numeric(Hmisc::cut2(predict_test, c(-Inf, optCuts$par, Inf)))
submission <- data.frame(Id=id, Response=preds)
require(readr)
write_csv(submission, "01102016_4.csv")
feature_imp <- xgb.importance( model = mod)
feature_imp$Feature <- as.numeric(feature_imp$Feature)
names <- c()
for(i in 1:length(feature_imp$Feature)){
names <- c(names(train)[feature_imp$Feature[i]], names)
}
feature_imp$names <- names
write_csv(feature_imp, "feature_imp_raw.csv")
require(doParallel)
rm(list = ls())
require(h2o); require(xgboost); require(readr)
train <- read_csv("D:\\kaggle\\PRUDENTIAL\\Data\\train_01242016.csv")
test <- read_csv("D:\\kaggle\\PRUDENTIAL\\Data\\test_01242016.csv")
response_1 <- read_csv("D:\\kaggle\\PRUDENTIAL\\Data\\response.csv")
response <- response_1$response
dataset_blend_train = matrix(0, nrow(train), 5)
dataset_blend_test_j = matrix(0, nrow(test), 5)
dataset_blend_test = matrix(0, nrow(test), 5)
for(j in 1:5)
{
print(paste("starting rf iteration ; number :", j))
set.seed(1*23*2016*j)
require(caret)
skf = createFolds(response, k = 5)
print(paste(nrow(dataset_blend_test_j),ncol(dataset_blend_test_j)))
# start fold loop------------------------------------------------------------------------------------
### Loop over the folds
i <- 0
for (sk in skf) {
i <- i + 1
print(paste("Fold", i))
### Extract and fit the train/test section for each fold
tmp_train <- unlist(skf[i])
x_train = train[-tmp_train,]
y_train = response[-tmp_train]
x_test  = train[tmp_train,]
y_test  = response[tmp_train]
require(h2o)
localH2O <- h2o.init(nthreads = -1, max_mem_size = '12g')
x_train$target <- as.factor(y_train)
train.hex <- as.h2o(localH2O, object = x_train)
test.hex <- as.h2o(localH2O, object = x_test)
myX <- names(train)
myY <- "target"
require(h2o)
localH2O <- h2o.init(nthreads = -1)
x_train$target <- (y_train)
train.hex <- as.h2o(localH2O, object = x_train)
test.hex <- as.h2o(localH2O, object = x_test)
myX <- names(train)
myY <- "target"
print(paste("training rf for iteration : ", j, "Fold ; number :", i))
test_rf <- h2o.randomForest(x = myX,
y = myY,
training_frame = train.hex,
ntrees = 1000,
max_depth = 10,
binomial_double_trees = T,
balance_classes = T
)
pred_rf <- h2o.predict(object = test_rf, newdata = test.hex)
pred_rf <- as.data.frame(pred_rf)
dataset_blend_train[tmp_train, j] <- pred_rf$predict
print(paste("predicting rf for test set iteration :", j, "; Fold :", i))
test.hex <- as.h2o(localH2O, object = test)
pred_rf <- h2o.predict(object = test_rf, newdata = test.hex)
pred_rf <- as.data.frame(pred_rf)
dataset_blend_test_j[, i] <- pred_rf$predict
}
dataset_blend_test[, j] <- rowMeans(dataset_blend_test_j)
}
require(readr)
write_csv(data.frame(dataset_blend_train), "D:\\kaggle\\PRUDENTIAL\\blend\\bag\\blend_train_rf_01232016.csv")
write_csv(data.frame(dataset_blend_test), "D:\\kaggle\\PRUDENTIAL\\blend\\bag\\blend_test_rf_01232016.csv")
rm(list = ls())
require(h2o); require(xgboost); require(readr)
train <- read_csv("D:\\kaggle\\PRUDENTIAL\\Data\\train_01242016.csv")
test <- read_csv("D:\\kaggle\\PRUDENTIAL\\Data\\test_01242016.csv")
response_1 <- read_csv("D:\\kaggle\\PRUDENTIAL\\Data\\response.csv")
response <- response_1$response
# create folds------------------------------------------------------------------------------------------
dataset_blend_train = matrix(0, nrow(train), 5)
dataset_blend_test_j = matrix(0, nrow(test), 5)
dataset_blend_test = matrix(0, nrow(test), 5)
for(j in 1:5)
{
print(paste("starting glm iteration ; number :", j))
set.seed(1*23*2016*j)
require(caret)
skf = createFolds(response, k = 5)
print(paste(nrow(dataset_blend_test_j),ncol(dataset_blend_test_j)))
# start fold loop------------------------------------------------------------------------------------
### Loop over the folds
i <- 0
for (sk in skf) {
i <- i + 1
print(paste("Fold", i))
### Extract and fit the train/test section for each fold
tmp_train <- unlist(skf[i])
x_train = train[-tmp_train,]
y_train = response[-tmp_train]
x_test  = train[tmp_train,]
y_test  = response[tmp_train]
require(h2o)
localH2O <- h2o.init(nthreads = -1, max_mem_size = '12g')
x_train$target <- y_train
train.hex <- as.h2o(localH2O, object = x_train)
test.hex <- as.h2o(localH2O, object = x_test)
myX <- names(train)
myY <- "target"
print(paste("training glm iteration :", j, "for Fold ; number :", i))
test_glm <- h2o.glm( x = myX,
y = myY,
training_frame = train.hex,
family = "poisson",
lambda_search = TRUE,
nlambdas = 5,
model_id = "glm_test",
solver = "L_BFGS",
#keep_cross_validation_predictions = T,
alpha = c(0, 0.1, 0.2, 0.3, 0.4, 0.6, 0.8, 1),
link = "log",
standardize = T
)
pred_glm <- h2o.predict(object = test_glm, newdata = test.hex)
pred_glm <- as.data.frame(pred_glm)
dataset_blend_train[tmp_train, j] <- pred_glm$predict
print(paste("predicting glm for test set iteration :", j, "; Fold :", i))
test.hex <- as.h2o(localH2O, object = test)
pred_glm <- h2o.predict(object = test_glm, newdata = test.hex)
pred_glm <- as.data.frame(pred_glm)
dataset_blend_test_j[, i] <- pred_glm$predict
}
dataset_blend_test[, j] <- rowMeans(dataset_blend_test_j)
}
require(readr)
write_csv(data.frame(dataset_blend_train), "D:\\kaggle\\PRUDENTIAL\\blend\\bag\\glm\\blend_train_glm_01232016.csv")
write_csv(data.frame(dataset_blend_test), "D:\\kaggle\\PRUDENTIAL\\blend\\bag\\glm\\blend_test_glm_01232016.csv")
h2o.shutdown()
require(h2o); require(xgboost); require(readr)
rm(list = ls())
train <- read_csv("D:\\kaggle\\PRUDENTIAL\\Data\\train_01242016.csv")
test <- read_csv("D:\\kaggle\\PRUDENTIAL\\Data\\test_01242016.csv")
response_1 <- read_csv("D:\\kaggle\\PRUDENTIAL\\Data\\response.csv")
response <- response_1$response
dataset_blend_train = matrix(0, nrow(train), 1)
dataset_blend_test_j = matrix(0, nrow(test), 3)
dataset_blend_test = matrix(0, nrow(test), 1)
j = 1
print(paste("starting gbm iteration ; number :", j))
set.seed(1*26*2016*j)
require(caret)
skf = createFolds(response, k = 3)
print(paste(nrow(dataset_blend_test_j),ncol(dataset_blend_test_j)))
# start fold loop------------------------------------------------------------------------------------
### Loop over the folds
i <- 0
for (sk in skf) {
i <- i + 1
print(paste("Fold", i))
### Extract and fit the train/test section for each fold
tmp_train <- unlist(skf[i])
x_train = train[-tmp_train,]
y_train = response[-tmp_train]
x_test  = train[tmp_train,]
y_test  = response[tmp_train]
require(h2o)
localH2O <- h2o.init(nthreads = -1, max_mem_size = '12g')
x_train$target <- as.factor(y_train)
train.hex <- as.h2o(localH2O, object = x_train)
test.hex <- as.h2o(localH2O, object = x_test)
myX <- names(train)
myY <- "target"
print(paste("training gbm for iteration :", j, " Fold ; number :", i))
test_gbm <- h2o.gbm(x = myX,
y = myY,
training_frame = train.hex,
ntrees =  2000,
max_depth = 20,
learn_rate = 0.014,
distribution= "multinomial",
min_rows = 9
)
pred_gbm <- h2o.predict(object = test_gbm, newdata = test.hex)
pred_gbm <- as.data.frame(pred_gbm)
dataset_blend_train[tmp_train, j] <- pred_gbm$predict
print(paste("predicting gbm for test set iteration :", j,  "; Fold :", i))
test.hex <- as.h2o(localH2O, object = test)
pred_gbm <- h2o.predict(object = test_gbm, newdata = test.hex)
pred_gbm <- as.data.frame(pred_gbm)
dataset_blend_test_j[, i] <- pred_gbm$predict
}
dataset_blend_test[, j] <- rowMeans(dataset_blend_test_j)
require(readr)
write_csv(data.frame(dataset_blend_train), "D:\\kaggle\\PRUDENTIAL\\blend\\bag\\gbm\\blend_train_gbm_01262016.csv")
write_csv(data.frame(dataset_blend_test), "D:\\kaggle\\PRUDENTIAL\\blend\\bag\\gbm\\blend_test_gbm_01262016.csv")
rm(list = ls())
require(h2o); require(xgboost); require(readr)
train <- read_csv("D:\\kaggle\\PRUDENTIAL\\Data\\train_01242016.csv")
test <- read_csv("D:\\kaggle\\PRUDENTIAL\\Data\\test_01242016.csv")
response_1 <- read_csv("D:\\kaggle\\PRUDENTIAL\\Data\\response.csv")
response <- response_1$response
localH2O <- h2o.init(nthreads = 4, max_mem_size = '12g')
dataset_blend_train = matrix(0, nrow(train), 2)
dataset_blend_test_j = matrix(0, nrow(test), 5) # should be equal to number of folds
dataset_blend_test = matrix(0, nrow(test), 2)
# start iteration loop---------------------------------------------------------------------------------
for(j in 1:2)
{
print(paste("starting gbm iteration ; number :", j))
set.seed(1*31*2016*j)
require(caret)
skf = createFolds(response, k = 5)
print(paste(nrow(dataset_blend_test_j),ncol(dataset_blend_test_j)))
# start fold loop------------------------------------------------------------------------------------
### Loop over the folds
i <- 0
for (sk in skf) {
i <- i + 1
print(paste("Fold", i))
### Extract and fit the train/test section for each fold
tmp_train <- unlist(skf[i])
x_train = train[-tmp_train,]
y_train = response[-tmp_train]
x_test  = train[tmp_train,]
y_test  = response[tmp_train]
require(h2o)
localH2O <- h2o.init(nthreads = 4, max_mem_size = '12g')
x_train$target <- as.factor(y_train)
train.hex <- as.h2o(localH2O, object = x_train)
test.hex <- as.h2o(localH2O, object = x_test)
myX <- names(train)
myY <- "target"
print(paste("training gbm for iteration :", j, " Fold ; number :", i))
test_gbm <- h2o.gbm(x = myX,
y = myY,
training_frame = train.hex,
ntrees =  2000,
max_depth = 20,
learn_rate = 0.014,
distribution= "multinomial",
min_rows = 9
)
pred_gbm <- h2o.predict(object = test_gbm, newdata = test.hex)
pred_gbm <- as.data.frame(pred_gbm)
dataset_blend_train[tmp_train, j] <- pred_gbm$predict
print(paste("predicting gbm for test set iteration :", j,  "; Fold :", i))
test.hex <- as.h2o(localH2O, object = test)
pred_gbm <- h2o.predict(object = test_gbm, newdata = test.hex)
pred_gbm <- as.data.frame(pred_gbm)
dataset_blend_test_j[, i] <- pred_gbm$predict
}
dataset_blend_test[, j] <- rowMeans(dataset_blend_test_j)
}
require(readr)
write_csv(data.frame(dataset_blend_train), "D:\\kaggle\\PRUDENTIAL\\blend\\bag\\gbm\\blend_train_gbm_01312016.csv")
write_csv(data.frame(dataset_blend_test), "D:\\kaggle\\PRUDENTIAL\\blend\\bag\\gbm\\blend_test_gbm_01312016.csv")
rm(list = ls())
require(h2o); require(xgboost); require(readr)
train <- read_csv("D:\\kaggle\\PRUDENTIAL\\Data\\New folder\\train_02072016.csv")
test <- read_csv("D:\\kaggle\\PRUDENTIAL\\Data\\New folder\\\\test_02072016.csv")
response_1 <- read_csv("D:\\kaggle\\PRUDENTIAL\\Data\\response.csv")
response <- response_1$response
j = 1
print(paste("starting rf iteration ; number :", j))
set.seed(1*23*2016*j)
require(caret)
skf = createFolds(response, k = 5)
print(paste(nrow(dataset_blend_test_j),ncol(dataset_blend_test_j)))
# start fold loop------------------------------------------------------------------------------------
### Loop over the folds
i <- 0
for (sk in skf) {
i <- i + 1
print(paste("Fold", i))
### Extract and fit the train/test section for each fold
tmp_train <- unlist(skf[i])
x_train = train[-tmp_train,]
y_train = response[-tmp_train]
x_test  = train[tmp_train,]
y_test  = response[tmp_train]
require(h2o)
localH2O <- h2o.init(nthreads = -1, max_mem_size = '12g')
x_train$target <- as.factor(y_train)
train.hex <- as.h2o(localH2O, object = x_train)
test.hex <- as.h2o(localH2O, object = x_test)
myX <- names(train)
myY <- "target"
require(h2o)
localH2O <- h2o.init(nthreads = -1)
x_train$target <- (y_train)
train.hex <- as.h2o(localH2O, object = x_train)
test.hex <- as.h2o(localH2O, object = x_test)
myX <- names(train)
myY <- "target"
print(paste("training rf for iteration : ", j, "Fold ; number :", i))
test_rf <- h2o.randomForest(x = myX,
y = myY,
training_frame = train.hex,
ntrees = 1000,
max_depth = 12,
binomial_double_trees = T,
balance_classes = T
)
pred_rf <- h2o.predict(object = test_rf, newdata = test.hex)
pred_rf <- as.data.frame(pred_rf)
dataset_blend_train[tmp_train, j] <- pred_rf$predict
print(paste("predicting rf for test set iteration :", j, "; Fold :", i))
test.hex <- as.h2o(localH2O, object = test)
pred_rf <- h2o.predict(object = test_rf, newdata = test.hex)
pred_rf <- as.data.frame(pred_rf)
dataset_blend_test_j[, i] <- pred_rf$predict
}
dataset_blend_test[, j] <- rowMeans(dataset_blend_test_j)
require(readr)
write_csv(data.frame(dataset_blend_train), "D:\\kaggle\\PRUDENTIAL\\blend\\bag\\blend_train_rf_02072016.csv")
write_csv(data.frame(dataset_blend_test), "D:\\kaggle\\PRUDENTIAL\\blend\\bag\\blend_test_rf_02072016.csv")
dataset_blend_train = matrix(0, nrow(train), 1)
dataset_blend_test_j = matrix(0, nrow(test), 5)
dataset_blend_test = matrix(0, nrow(test), 1)
j = 1
print(paste("starting rf iteration ; number :", j))
set.seed(02*07*2016*j)
require(caret)
skf = createFolds(response, k = 5)
print(paste(nrow(dataset_blend_test_j),ncol(dataset_blend_test_j)))
# start fold loop------------------------------------------------------------------------------------
### Loop over the folds
i <- 0
for (sk in skf) {
i <- i + 1
print(paste("Fold", i))
### Extract and fit the train/test section for each fold
tmp_train <- unlist(skf[i])
x_train = train[-tmp_train,]
y_train = response[-tmp_train]
x_test  = train[tmp_train,]
y_test  = response[tmp_train]
require(h2o)
localH2O <- h2o.init(nthreads = -1, max_mem_size = '12g')
x_train$target <- as.factor(y_train)
train.hex <- as.h2o(localH2O, object = x_train)
test.hex <- as.h2o(localH2O, object = x_test)
myX <- names(train)
myY <- "target"
require(h2o)
localH2O <- h2o.init(nthreads = -1)
x_train$target <- (y_train)
train.hex <- as.h2o(localH2O, object = x_train)
test.hex <- as.h2o(localH2O, object = x_test)
myX <- names(train)
myY <- "target"
print(paste("training rf for iteration : ", j, "Fold ; number :", i))
test_rf <- h2o.randomForest(x = myX,
y = myY,
training_frame = train.hex,
ntrees = 1000,
max_depth = 12,
binomial_double_trees = T,
balance_classes = T
)
pred_rf <- h2o.predict(object = test_rf, newdata = test.hex)
pred_rf <- as.data.frame(pred_rf)
dataset_blend_train[tmp_train, j] <- pred_rf$predict
print(paste("predicting rf for test set iteration :", j, "; Fold :", i))
test.hex <- as.h2o(localH2O, object = test)
pred_rf <- h2o.predict(object = test_rf, newdata = test.hex)
pred_rf <- as.data.frame(pred_rf)
dataset_blend_test_j[, i] <- pred_rf$predict
}
dataset_blend_test[, j] <- rowMeans(dataset_blend_test_j)
require(readr)
write_csv(data.frame(dataset_blend_train), "D:\\kaggle\\PRUDENTIAL\\blend\\bag\\blend_train_rf_02072016.csv")
write_csv(data.frame(dataset_blend_test), "D:\\kaggle\\PRUDENTIAL\\blend\\bag\\blend_test_rf_02072016.csv")
