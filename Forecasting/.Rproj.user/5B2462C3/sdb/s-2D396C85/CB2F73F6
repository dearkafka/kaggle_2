{
    "contents" : "#10222015--------------------------------------------------------------------------------\n\nlibrary(readr)\n\nlibrary(xgboost)\n\nset.seed(28071993)\n\ncat(\"reading the train and test data\\n\")\n\ntrain <- read_csv(\"D:/kaggle/Forecasting/DATA/train.csv\")\n\ntest  <- read_csv(\"D:/kaggle/Forecasting/DATA/test.csv\")\n\nstore <- read_csv(\"D:/kaggle/Forecasting/DATA/store.csv\")\n\n# removing the date column (since elements are extracted) and also StateHoliday which has a lot of NAs (may add it back in later)\n\ntrain <- merge(train,store)\n\ntest <- merge(test,store)\n\n# There are some NAs in the integer columns so conversion to zero\n\ntrain[is.na(train)]   <- -1\n\ntest[is.na(test)]   <- -1\n\nnames(train)\n\nstr(train)\n\nsummary(train)\n\nnames(test)\n\nstr(test)\n\nsummary(test)\n\n# looking at only stores that were open in the train set\n\n# may change this later\n\ntrain <- train[ which(train$Open=='1'),]\n\ntrain <- train[ which(train$Sales!='0'),]\n\n# seperating out the elements of the date column for the train set\n\ntrain$month <- as.integer(format(train$Date, \"%m\"))\n\ntrain$year <- as.integer(format(train$Date, \"%y\"))\n\ntrain$day <- as.integer(format(train$Date, \"%d\"))\n\n# removing the date column (since elements are extracted) and also StateHoliday which has a lot of NAs (may add it back in later)\n\ntrain <- train[,-c(3,8)]\n\n# seperating out the elements of the date column for the test set\n\ntest$month <- as.integer(format(test$Date, \"%m\"))\n\ntest$year <- as.integer(format(test$Date, \"%y\"))\n\ntest$day <- as.integer(format(test$Date, \"%d\"))\n\n# removing the date column (since elements are extracted) and also StateHoliday which has a lot of NAs (may add it back in later)\n\ntest <- test[,-c(4,7)]\n\nfeature.names <- names(train)[c(1,2,5:19)]\n\nfeature.names\n\ncat(\"assuming text variables are categorical & replacing them with numeric ids\\n\")\n\nfor (f in feature.names) {\n  \n  if (class(train[[f]])==\"character\") {\n    \n    levels <- unique(c(train[[f]], test[[f]]))\n    \n    train[[f]] <- as.integer(factor(train[[f]], levels=levels))\n    \n    test[[f]]  <- as.integer(factor(test[[f]],  levels=levels))\n  }\n}\n\nresponse <- train$Sales\n\ntrain$Sales <- NULL\n\nsplit = createDataPartition(train_target, p = 0.9, list = F)\n\nresponse_val <- response[-split]\n\nresponse_train <- response[split]\n\ntmp <- train[,feature.names]\n\ndval <- xgb.DMatrix(data=data.matrix(tmp[-split, ]), label = log(response_val+1))\n\ndtrain <- xgb.DMatrix(data=data.matrix(tmp[split, ]), label = log(response_train+1))\n\nwatchlist<-list(val=dval,train=dtrain)\n\nRMPSE<- function(preds, dtrain) {\n  \n  labels <- getinfo(dtrain, \"label\")\n  \n  elab <- exp(as.numeric(labels))-1\n  \n  epreds <- exp(as.numeric(preds))-1\n  \n  err <- sqrt(mean((epreds/elab-1)^2))\n  \n  return(list(metric = \"RMPSE\", value = err))\n}\n\nparam <- list(  objective           = \"reg:linear\", \n                \n                booster = \"gbtree\",\n                \n                eta                 = 0.01, \n                \n                max_depth           = 20, \n                \n                subsample           = 0.7, \n                \n                colsample_bytree    = 0.7 \n                \n                # alpha = 0.0001, \n                \n                # lambda = 1\n)\n\ncl <- makeCluster(2); registerDoParallel(cl)\n\nclf <- xgb.train(   params              = param, \n                    \n                    data                = dtrain, \n                    \n                    nrounds             = 1000, #300, #280, #125, #250, # changed from 300\n                    \n                    verbose             = 1,\n                    \n                    nthread             = 2,\n                    \n                    \n                    \n                    watchlist           = watchlist,\n                    \n                    maximize            = FALSE,\n                    \n                    feval=RMPSE\n)\n\n\npred <- exp(predict(clf, data.matrix(test[,feature.names]))) -1\n\nsubmission <- data.frame(Id=test$Id, Sales=pred)\n\nwrite_csv(submission, \"D:/kaggle/Forecasting/submission/1022015.csv\")\n\n######################################################################################\n\n\n\n\n#10232015------------------------------------------------------------------------------\n\nlibrary(data.table)  \n\nlibrary(h2o)\n\ntrain <- fread(\"../input/train.csv\",stringsAsFactors = T)\n\ntest  <- fread(\"../input/test.csv\",stringsAsFactors = T)\n\nstore <- fread(\"../input/store.csv\",stringsAsFactors = T)\n\ntrain <- train[Sales > 0,]  ## We are not judged on 0 sales records in test set\n\ntrain <- merge(train,store,by=\"Store\")\n\ntest <- merge(test,store,by=\"Store\")\n\ntrain[,Date:=as.Date(Date)]\n\ntest[,Date:=as.Date(Date)]\n\n# seperating out the elements of the date column for the train set\n\ntrain[,month:=as.integer(format(Date, \"%m\"))]\n\ntrain[,year:=as.integer(format(Date, \"%y\"))]\n\ntrain[,Store:=as.factor(as.numeric(Store))]\n\ntest[,month:=as.integer(format(Date, \"%m\"))]\n\ntest[,year:=as.integer(format(Date, \"%y\"))]\n\ntest[,Store:=as.factor(as.numeric(Store))]\n\ntrain[,logSales:=log1p(Sales)]\n\nh2o.init(nthreads=-1,max_mem_size='6G')\n\ntrainHex<-as.h2o(train)\n\nfeatures<-colnames(train)[!(colnames(train) %in% c(\"Id\",\"Date\",\"Sales\",\"logSales\",\"Customers\"))]\n\nrfHex <- h2o.randomForest(x=features,\n                          y=\"logSales\", \n                          ntrees = 100,\n                          max_depth = 30,\n                          nbins_cats = 1115, ## allow it to fit store ID\n                          training_frame=trainHex)\n\ntestHex<-as.h2o(test)\n\npredictions<-as.data.frame(h2o.predict(rfHex,testHex))\n\npred <- expm1(predictions[,1])\n\nsubmission <- data.frame(Id=test$Id, Sales=pred)\n\nwrite_csv(submission, \"D:/kaggle/Forecasting/submission/1023015.csv\")\n\nmetric=function(y,yhat)\n  \n{\n  \n  y=exp(y)-1\n  \n  yhat=exp(yhat)-1\n  \n  sum_squared=0\n  \n  for( i in c(1:length(yhat)))\n    \n  {\n    \n    prop_squared=((y[i]-yhat[i])/y[i])^2\n    \n    sum_squared=sum_squared+prop_squared\n    \n  }\n  \n  err=sqrt((1/length(yhat))*sum_squared)\n  \n  return(err)\n  \n}\n\n#######################################################################################\n\n\n#10232015_1-----------------------------------------------------------------------------\n\nlibrary(readr); library(xgboost); library(data.table); require(sqldf)\n\nset.seed(28071993)\n\ntrain <- fread(\"D:/kaggle/Forecasting/DATA/train.csv\")\n\nstore <- fread(\"D:/kaggle/Forecasting/DATA/store.csv\")\n\ntest <- fread(\"D:/kaggle/Forecasting/DATA/test.csv\")\n\ntrain <- merge(train,store, by = \"Store\")\n\ntest <- merge(test,store, by = \"Store\")\n\nsetdiff(names(train), names(test))\n\ntest[, `:=`(Sales = \"NA\", Customers = \"NA\" )]\n\ntrain[, `:=`(Id = 1:nrow(train) )]\n\ntmp <- rbind(train, test)\n\ntmp[, Date := as.Date(Date)]\n\ntmp[, `:=`( month = as.integer(format(Date, \"%m\")), \n            \n            year = as.integer(format(Date, \"%y\")),\n            \n            day = as.integer(format(Date, \"%d\"))\n            \n)]\n\nfactors <- names(tmp)[!(names(tmp) %in% c(\"Date\", \"Sales\", \"Customers\", \n                                          \"CompetitionDistance\", \"Id\"))]\ntmp <- data.frame(tmp)\n\ntmp_original <- tmp\n\nfor( i in factors){\n  \n  tmp[, i] <- as.factor(tmp[, i])\n  \n  print(paste(i, \":\", length(table(tmp[i]))))\n}\n\n##########################################################################################\n\n#using owen`s Amazon code approach\n\nmy.f2cnt <- function(th2, vn1, vn2, filter=TRUE) {\n  \n  df <- data.frame(f1=th2[,vn1], f2=th2[,vn2], filter=filter)\n  \n  sum1 <- sqldf(\"select f1, f2, count(*) as cnt \n                \n                from df \n                \n                where filter=1 \n                \n                group by 1,2\")\n  \n  tmp <- sqldf(\"select b.cnt \n               \n               from df a left join sum1 b \n               \n               on a.f1=b.f1 and a.f2=b.f2\")\n  \n  tmp$cnt[is.na(tmp$cnt)] <- 0\n  \n  return(tmp$cnt)\n  \n}\n\n#3 way count\n\nmy.f3cnt<-function(th2, vn1, vn2, vn3, filter=TRUE) {\n  \n  df<-data.frame(f1=th2[,vn1], f2=th2[,vn2], f3=th2[, vn3], filter=filter)\n  \n  sum1<-sqldf(\"select f1, f2, f3, count(*) as cnt \n              \n              from df \n              \n              where filter=1 \n              \n              group by 1,2, 3\")\n  \n  tmp<-sqldf(\"select b.cnt \n             \n             from df a left join sum1 b \n             \n             on a.f1=b.f1 and a.f2=b.f2 and a.f3=b.f3\")\n  \n  tmp$cnt[is.na(tmp$cnt)]<-0\n  \n  return(tmp$cnt)\n  \n}\n\n#####################################################################################################\n\n#2 way count--------------------------------------------------------------------\n\nnms <- combn(factors, 2)\n\ndim(nms)\n\nnms_df <- data.frame(nms) \n\nlen = length(names(nms_df))\n\nfor (i in 1:len) {\n  \n  nms_df[, i] <- as.character(nms_df[, i])\n  \n}\n\ntmp_count <- data.frame(id = 1:dim(tmp)[1])\n\nfor(i in 1:dim(nms_df)[2]){\n  \n  print(((i / dim(nms_df)[2]) * 100 ))\n  \n  tmp_count[, paste(i, \"_two\", sep=\"\")] <- my.f2cnt(th2 = tmp, \n                                                    \n                                                    vn1 = nms_df[1,i], \n                                                    \n                                                    vn2 = nms_df[2,i] )\n  \n}\n\n#3 way count--------------------------------------------------------------------\n\nstart <- Sys.time()\n\nnms <- combn(factors, 3)\n\ndim(nms)\n\nnms_df <- data.frame(nms)\n\nlen = length(names(nms_df))\n\nfor (i in 1:len) {\n  \n  print(paste0(( i / len) *100, \"%\"))\n  \n  nms_df[, i] <- as.character(nms_df[, i])\n  \n}\n\nfor(i in 1:dim(nms_df)[2]){\n  \n  print((i / dim(nms_df)[2]) * 100)\n  \n  tmp_count[, paste(i, \"_three\", sep=\"\")] <- my.f3cnt(th2 = tmp, \n                                                      \n                                                      vn1 = nms_df[1,i], \n                                                      \n                                                      vn2 = nms_df[2,i], \n                                                      \n                                                      vn3 = nms_df[3,i])\n  \n}\n\ntime_taken <- Sys.time() - start \n\ntmp_new = cbind.data.frame(tmp_original, tmp_count)\n\n\n\n###############################################################################\n\ntrain <-  tmp_new[1:1017209,  ]\n\ntest <- tmp_new[(1017209+1) : nrow(tmp_new), ]\n\nrm(nms); rm(nms_df); rm(store); rm(tmp); rm(factor_col)\nrm(factors); rm(i); rm(len); rm(removecols); rm(tmp_count); rm(tmp_original)\nrm(tmp_new)\n\ngc()\n\nremovecols <- c(\"Id\",\"Date\",\"Sales\",\"Customers\")\n\nfeature.names <- colnames(train)[!(colnames(train) %in% removecols)]\n\ntrain = setDT(train)\n\ntrain <- train[Open == 1]\n\ntrain <- train[Sales != 0]\n\ntrain <- as.data.frame(train)\n\n#train <- train[ which(train$Open=='1'),]\n\n#train <- train[ which(train$Sales!='0'),]\n\ntrain[is.na(train)] <- 0\n\ntest[is.na(test)] <- 1\n\nfor (f in names(train)) {\n  \n  if (class(train[[f]])==\"character\") {\n    \n    levels <- unique(c(train[[f]], test[[f]]))\n    \n    train[[f]] <- as.integer(factor(train[[f]], levels=levels))\n    \n    test[[f]]  <- as.integer(factor(test[[f]],  levels=levels))\n    \n  }\n}\n\nresponse <- train$Sales\n\nsplit = createDataPartition(response, p = 0.9, list = F)\n\nresponse_val <- response[-split]\n\nresponse_train <- response[split]\n\ntmp <- train[,feature.names]\n\ndval <- xgb.DMatrix(data=data.matrix(tmp[-split, ]), label = (response_val))\n\ndtrain <- xgb.DMatrix(data=data.matrix(tmp[split, ]), label = (response_train))\n\nwatchlist<-list(val=dval,train=dtrain)\n\nparam <- list(  objective           = \"reg:linear\", \n                booster = \"gbtree\",\n                eta                 = 0.02, # 0.06, #0.01,\n                max_depth           = 10, #changed from default of 8\n                subsample           = 0.9, # 0.7\n                colsample_bytree    = 0.7 # 0.7\n                #num_parallel_tree   = 2\n                # alpha = 0.0001, \n                # lambda = 1\n)\n\nRMPSE<- function(preds, dtrain) {\n  \n  labels <- getinfo(dtrain, \"label\")\n  \n  elab <- exp(as.numeric(labels))-1\n  \n  epreds <- exp(as.numeric(preds))-1\n  \n  err <- sqrt(mean((epreds/elab-1)^2))\n  \n  return(list(metric = \"RMPSE\", value = err))\n}\n\n\ngc()\nlibrary(doParallel)\ncl <- makeCluster(2); registerDoParallel(cl)\n\nclf <- xgb.train(   params              = param, \n                    data                = dtrain, \n                    nrounds             = 3000, #300, #280, #125, #250, # changed from 300\n                    verbose             = 2,\n                    early.stop.round    = 600,\n                    watchlist           = watchlist,\n                    maximize            = T,\n                    feval=RMPSE\n)\n\npred1 <- exp(predict(clf, data.matrix(test[,feature.names]))) -1\n\nsubmission <- data.frame(Id=test$Id, Sales=pred1)\n\nwrite_csv(submission, \"10252015.csv\")\n\n\n#10252015_1-------------------------------------------------------------------------------\n\n# changed n_rounds to 15000, LB score decreased ",
    "created" : 1445510451017.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1444669751",
    "id" : "CB2F73F6",
    "lastKnownWriteTime" : 1445910935,
    "path" : "D:/kaggle/Forecasting/backup_wrkngFile.R",
    "project_path" : "backup_wrkngFile.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "type" : "r_source"
}