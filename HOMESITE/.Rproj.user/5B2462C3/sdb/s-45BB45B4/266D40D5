{
    "contents" : "# begin blend-------------------------------------------------------------------------------------------\n\n\n### Returns train inidices for n_folds using StratifiedKFold\n\nskf = createFolds(response, k = 3)\n\n\n### Create a list of models to run\n\nclfs <- c(\"glm\", \"gbm\", \"rf\")\n\n\n### Pre-allocate the data\n\n### For each model, add a column with N rows for each model\n\n\ndataset_blend_train = matrix(0, nrow(train), length(clfs))\n\ndataset_blend_test  = matrix(0, nrow(test), length(clfs))\n\n\n\n### Loop over the models------------------------------------------------------------------------------\n\nj <- 0 \n\nfor (clf in clfs)\n  \n{\n  \n  j <- j + 1\n  \n  print(paste(j,clf))\n  \n  \n  ### Create a tempory array that is (Holdout_Size, N_Folds).\n  \n  ### Number of testing data x Number of folds , we will take the mean of the predictions later\n  \n  dataset_blend_test_j = matrix(0, nrow(test), length(skf))\n  \n  print(paste(nrow(dataset_blend_test_j),ncol(dataset_blend_test_j)))\n  \n  \n  ### Loop over the folds\n  \n  i <- 0\n  \n  for (sk in skf) {\n    \n    i <- i + 1\n    \n    print(paste(\"Fold\", i))\n    \n    \n    ### Extract and fit the train/test section for each fold\n    \n    tmp_train <- unlist(skf[i])\n    \n    x_train = train[-tmp_train,]\n    \n    y_train = response[-tmp_train]\n    \n    x_test  = train[tmp_train,]\n    \n    y_test  = response[tmp_train] # write a scoring method to score auc from y_test\n    \n    \n    ### fit xgboost--------------------------------------------------------------------------------------\n    \n#     if(clf == \"xgboost_1\"){\n#       \n#       \n#       \n#       feature.names <- names(train)\n      \n      # h<-sample(nrow(train),2000)\n      \n      # dval<-xgb.DMatrix(data=data.matrix(train[h,]),label=response[h])\n      \n      # dtrain<-xgb.DMatrix(data=data.matrix(train[-h,]),label=response[-h])\n      \n      # dtrain<-xgb.DMatrix(data=data.matrix(x_train),label=y_train, missing = NaN)\n      \n      # watchlist<-list(val=dval,train=dtrain)\n      \n#       param <- list(  objective           = \"binary:logistic\",\n#                       \n#                       booster = \"gbtree\",\n#                       \n#                       eval_metric = \"auc\",\n#                       \n#                       eta                 = 0.023, # 0.06, #0.01,\n#                       \n#                       max_depth           = 6, #changed from default of 8\n#                       \n#                       subsample           = 0.83, # 0.7\n#                       \n#                       colsample_bytree    = 0.77, # 0.7\n#                       \n#                       num_parallel_tree = 2,\n#                       \n#                       min_child_weight = 5\n#                       \n#       )\n#       \n#       start <- Sys.time()\n#       \n#       require(doParallel)\n#       \n#       cl <- makeCluster(2); registerDoParallel(cl)\n#       \n#       set.seed(1142016)\n#       \n#       mod <- xgb.train(   params              = param,\n#                           \n#                           data                = dtrain,\n#                           \n#                           nrounds             = 2000,\n#                           \n#                           verbose             = 1,  #1\n#                           \n#                           #early.stop.round    = 150,\n#                           \n#                           # watchlist           = watchlist,\n#                           \n#                           maximize            = T,\n#                           \n#                           nthread = 2)\n#       \n#       \n#       dataset_blend_train[tmp_train, j] <- predict(mod, data.matrix(x_test), missing = NaN)\n#       \n#       xgb.save(clf, \"D:\\\\kaggle\\\\HOMESITE\\\\ensemble\\\\011402016_xgb.R\")\n#       \n#     }\n    \n    \n    # fit rf---------------------------------------------------------------------------------------------\n    \n    if (clf == \"rf\"){\n      \n      # combining them in a df for maintaining format\n      \n      require(h2o)\n      \n      localH2O <- h2o.init(max_mem_size = \"10g\")\n      \n      x_train$target <- y_train\n      \n      train.hex <- as.h2o(localH2O, object = x_train)\n      \n      test.hex <- as.h2o(localH2O, object = x_test)\n      \n      \n      myX <- names(train)\n      \n      myY <- \"target\"\n      \n      \n      test_rf <- h2o.randomForest(x = myX,\n                                  \n                                  y = myY,\n                                  \n                                  training_frame = train.hex,\n                                  \n                                  model_id = \"rf_1152016\", \n                                  \n                                  ntrees = 1000, \n                                  \n                                  max_depth = 10, \n                                  \n                                  binomial_double_trees = T, \n                                  \n                                  balance_classes = T, \n                                  \n                                  seed = 01152016\n                                  \n      )\n      \n      pred_rf <- h2o.predict(object = test_rf, newdata = test.hex)\n      \n      pred_rf <- as.data.frame(pred_rf)\n      \n      \n      dataset_blend_train[tmp_train, j] <- pred_rf$predict\n      \n      \n      \n    }\n    \n    \n    \n    # fit gbm--------------------------------------------------------------------------------------------\n    \n    else if(clf == \"gbm\"){\n      \n      require(h2o)\n      \n      localH2O <- h2o.init(max_mem_size = \"10g\")\n      \n      x_train$target <- y_train\n      \n      train.hex <- as.h2o(localH2O, object = x_train)\n      \n      test.hex <- as.h2o(localH2O, object = x_test)\n      \n      \n      myX <- names(train)\n      \n      myY <- \"target\"\n      \n      \n      \n      test_gbm <- h2o.gbm(x = myX,\n                          \n                          y = myY,\n                          \n                          training_frame = train.hex,\n                          \n                          model_id = \"gbm_01152016\", \n                          \n                          ntrees =  1000, \n                          \n                          max_depth = 20, \n                          \n                          learn_rate = 0.014, \n                          \n                          seed = 01152016, \n                          \n                          balance_classes = T, \n                          \n                          min_rows = 9,\n                          \n                          family = \"binomial\"\n      )\n      \n      \n      pred_gbm <- h2o.predict(object = test_gbm, newdata = test.hex)\n      \n      pred_gbm <- as.data.frame(pred_gbm)\n      \n      \n      dataset_blend_train[tmp_train, j] <- pred_gbm$predict\n      \n      \n      \n    }\n    \n    \n    \n    # fit glm--------------------------------------------------------------------------------------------\n    \n    else if(clf == \"glm\"){\n      \n      require(h2o)\n      \n      localH2O <- h2o.init(max_mem_size = \"10g\")\n      \n      x_train$target <- y_train\n      \n      train.hex <- as.h2o(localH2O, object = x_train)\n      \n      test.hex <- as.h2o(localH2O, object = x_test)\n      \n      \n      myX <- names(train)\n      \n      myY <- \"target\"\n      \n      \n      test_glm <- h2o.glm( x = myX,\n                           \n                           y = myY,\n                           \n                           training_frame = train.hex,\n                           \n                           family = \"binomial\",\n                           \n                           lambda_search = TRUE,\n                           \n                           nlambdas = 5, \n                           \n                           model_id = \"glm_test\", \n                           \n                           solver = \"L_BFGS\",\n                           \n                           keep_cross_validation_predictions = T,\n                           \n                           alpha = c(0, 0.1, 0.2, 0.3, 0.4, 0.6, 0.8, 1), \n                           \n                           link = \"logit\", \n                           \n                           standardize = T\n                           \n      )\n      \n      pred_glm <- h2o.predict(object = test_glm, newdata = test.hex)\n      \n      pred_glm <- as.data.frame(pred_glm)\n      \n      \n      dataset_blend_train[tmp_train, j] <- pred_glm$p0\n      \n      \n      \n    }\n    \n    \n    # predict xgboost for test set---------------------------------------------------------------------\n    \n#     if(clf == \"xgboost_1\")\n#       \n#     {\n#       \n#       print(paste(\"predicting xgboost for test set ; Fold :\", i))\n#       \n#       dataset_blend_test_j[, i] <- predict(mod, data.matrix(test), missing = NaN)\n#       \n#     }\n    \n    # predict rf for test set------------------------------------------------------------------\n    \n    \n    if(clf == \"rf\"){\n      \n      print(paste(\"predicting rf for test set ; Fold :\", i))\n      \n      test.hex <- as.h2o(localH2O, object = test)\n      \n      pred_rf <- h2o.predict(object = test_rf, newdata = test.hex)\n      \n      pred_rf <- as.data.frame(pred_rf)\n      \n      \n      dataset_blend_test_j[, i] <- pred_rf$predict\n      \n    }\n    \n    # predict gbm for test set---------------------------------------------------------------\n    \n    \n    else if(clf == \"gbm\"){\n      \n      print(paste(\"predicting gbm for test set ; Fold :\", i))\n      \n      test.hex <- as.h2o(localH2O, object = test)\n      \n      pred_gbm <- h2o.predict(object = test_gbm, newdata = test.hex)\n      \n      pred_gbm <- as.data.frame(pred_gbm)\n      \n      \n      dataset_blend_test_j[, i] <- pred_gbm$predict\n    }\n    \n    \n    # predict glm for test set------------------------------------------------------------------\n    \n    else if(clf == \"glm\"){\n      \n      print(paste(\"predicting glm for test set ; Fold :\", i))\n      \n      test.hex <- as.h2o(localH2O, object = test)\n      \n      pred_glm <- h2o.predict(object = test_glm, newdata = test.hex)\n      \n      pred_glm <- as.data.frame(pred_glm)\n      \n      \n      dataset_blend_test_j[, i] <- pred_glm$p0\n      \n      \n      \n    }\n    \n  }\n  \n  dataset_blend_test[,j] = rowMeans(dataset_blend_test_j)  \n  \n}\n\n\nwrite_csv(as.data.frame(dataset_blend_test), \"dataset_blend_test.csv\")\n\nwrite_csv(as.data.frame(dataset_blend_train), \"dataset_blend_train.csv\")\n",
    "created" : 1454768742540.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3419098043",
    "id" : "266D40D5",
    "lastKnownWriteTime" : 1452953768,
    "path" : "D:/kaggle/HOMESITE/ENSEMBLE.R",
    "project_path" : "ENSEMBLE.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 9,
    "source_on_save" : false,
    "type" : "r_source"
}