{
    "contents" : "require(doParallel); require(xgboost); require(caret); require(readr)\n\ndetectCores()\n\ntrain <- read_csv(\"train_01262016.csv\")\n\ntest <- read_csv(\"test_01262016.csv\")\n\nresponse_1 <- read_csv(\"response_01262016.csv\")\n\nresponse <- response_1$response\n\n\n# create folds------------------------------------------------------------------------------------------\n\ndataset_blend_train = matrix(0, nrow(train), 5)\n\ndataset_blend_test_j = matrix(0, nrow(test), 5)\n\ndataset_blend_test = matrix(0, nrow(test), 5)\n\n# start iteration loop---------------------------------------------------------------------------------\n\nfor(j in 1:5)\n  \n{\n\n  print(paste(\"starting xgboost iteration ; number :\", j))\n  \n  set.seed(1*26*2016*j)\n  \n  require(caret)\n  \n  skf = createFolds(response, k = 5)\n  \n  print(paste(nrow(dataset_blend_test_j),ncol(dataset_blend_test_j)))\n  \n  # start fold loop------------------------------------------------------------------------------------\n  \n  ### Loop over the folds\n  \n  i <- 0\n  \n  Sys.time()\n  \n  for (sk in skf) {\n    \n    i <- i + 1\n    \n    print(paste(\"Fold\", i))\n    \n    ### Extract and fit the train/test section for each fold\n    \n    tmp_train <- unlist(skf[i])\n    \n    x_train = train[-tmp_train,]\n    \n    y_train = response[-tmp_train]\n    \n    x_test  = train[tmp_train,]\n    \n    y_test  = response[tmp_train]\n    \n    feature.names <- names(train)\n    \n    dtrain<-xgb.DMatrix(data=data.matrix(x_train),label=y_train)\n\n    param <- list(  objective           = \"binary:logistic\", \n                    \n                    booster = \"gbtree\",\n                    \n                    eval_metric = \"auc\",\n                    \n                    eta                 = 0.023, # 0.06, #0.01,\n                    \n                    max_depth           = 6, #changed from default of 8\n                    \n                    subsample           = 0.83, # 0.7\n                    \n                    colsample_bytree    = 0.77, # 0.7\n                    \n                    num_parallel_tree = 2\n                    \n    )\n    \n    start <- Sys.time()\n    \n    require(doParallel)\n    \n    cl <- makeCluster(detectCores()) \n    \n    registerDoParallel(cl)\n    \n    # start training------------------------------------------------------------------------------\n    \n    print(paste(\"training xgboost for iteration :\", j, \"Fold ; number :\", i))\n    \n    mod <- xgb.train(   params              = param,\n                        \n                        data                = dtrain,\n                        \n                        nrounds             = 3000,\n                        \n                        verbose             = 1,  \n                        \n                        maximize            = T,\n                        \n                        nthread = detectCores())\n    \n    dataset_blend_train[tmp_train, j] <- predict(mod, data.matrix(x_test))\n    \n    \n    \n    print(paste(\"predicting xgboost for test set iteration:\", j, \" ; Fold :\", i))\n    \n    dataset_blend_test_j[, i] <- predict(mod, data.matrix(test))\n    \n  }\n  \n  dataset_blend_test[, j] <- rowMeans(dataset_blend_test_j)\n  \n}\n\nrequire(readr)\n\nwrite_csv(data.frame(dataset_blend_train), \"blend_train_xgb_01252016.csv\")\n\nwrite_csv(data.frame(dataset_blend_test_j), \"blend_test_xgb_01252016.csv\")",
    "created" : 1453783109096.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2238578535",
    "id" : "2FEF9E7A",
    "lastKnownWriteTime" : 1453786082,
    "path" : "D:/kaggle/HOMESITE/aws.R",
    "project_path" : "aws.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_source"
}