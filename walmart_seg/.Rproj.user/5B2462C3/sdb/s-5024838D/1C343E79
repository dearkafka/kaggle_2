{
    "contents" : "rm(test_raw); rm(train_raw); rm(cl);rm(class_new); rm(class_old); rm(depth); rm(cnames); rm(dtrain); rm(dval)\n\nrm(eta); rm(f); rm(feature.names); rm(i); rm(j); rm(len); rm(levels); rm(nam); rm(name); rm(numberOfClasses)\n\nrm(start); rm(time_taken); rm(watchlist); rm(my.f2cnt); rm(my.f3cnt); rm(tmp_str); rm(tmp_new); rm(lnth)\n\n\n\n#feature hashing\n\ntrain_hash <- train\n\ntrain_hash[is.na(train_hash)] <- 0\n\nsplit <- createDataPartition(y = train_hash$TripType, p = 0.9, list = F) \n\ntraining <- train_hash[split,]\n\nvalidation <- train_hash[-split,]\n\ntraining_hash = hashed.model.matrix(~., data=training[,feature.names],  hash.size=2^16,  \n                                    \n                                    transpose=FALSE, create.mapping=TRUE, is.dgCMatrix = TRUE)\n\nvalidation_hash = hashed.model.matrix(~., data=validation[,feature.names],  hash.size=2^16,  \n                                      \n                                      transpose=FALSE, create.mapping=TRUE, is.dgCMatrix = TRUE)\n\nresponse_val <- train_hash$TripType[-split]\n\nresponse_train <- train_hash$TripType[split]\n\ndval <- xgb.DMatrix(data=validation_hash, label = response_val )\n\ndtrain <- xgb.DMatrix(data=training_hash,  label = response_train)\n\nwatchlist <- list(val=dval, train=dtrain)\n\nclf <- xgb.train(params = param, data = dtrain, nrounds = 500, watchlist = watchlist,\n                 \n                 verbose = 1, maximize = T)\n\n\n\n#################################################################################################\n\n#normal training method\n\nfeature.names <- names(train)[-c(179) ]\n\ntra <- train[, feature.names]\n\nsplit <- createDataPartition(y = train_raw$TripType, p = 0.9, list = F) \n\nresponse_val <- train_raw$TripType[-split]\n\nresponse_train <- train_raw$TripType[split]\n\ndval <- xgb.DMatrix( data = data.matrix(tra[-split,]),  label = response_val )\n\ndtrain <- xgb.DMatrix( data = data.matrix(tra[split,]), label = response_train)\n\n#sum(is.na(train)); sum(is.na(test)) # test if found NA error\n\nwatchlist <- list(val=dval, train=dtrain)\n\n#basic training----------------------------------------------------------------------------------\n\nnumberOfClasses <- max(train_raw$TripType) + 1\n\nparam <- list(objective = \"multi:softprob\",\n              \n              eval_metric = \"mlogloss\",\n              \n              num_class = numberOfClasses,\n              \n              max_depth = 16,\n              \n              eta = 0.03,\n              \n              colsample_bytree = 0.8,\n              \n              subsample = 0.8\n              )\n\ngc()\n\n\ncl <- makeCluster(detectCores()); registerDoParallel(cl)\n\n\nstart <- Sys.time()\n\n\n#############################################################################################################\n\n\nclf <- xgb.train(params = param, data = dtrain, nrounds = 100, watchlist = watchlist,\n                 \n                 verbose = 1, maximize = T, nthread = 2)\n\n\ntime_taken <- Sys.time() - start\n\n\n#############################################################################################################\n\n\n#grid search\n\nfor (depth in c(10, 15, 20,25)) {\n  \n  for(eta in c(0.03, 0.02, 0.01)){\n    \n    # train\n    param <- list(objective = \"multi:softprob\",\n                  \n                  eval_metric = \"mlogloss\",\n                  \n                  num_class = numberOfClasses,\n                  \n                  max_depth = depth ,\n                  \n                  eta = eta\n                  \n    )\n    \n    \n    clf <- xgb.train(params = param, data = dtrain, watchlist = watchlist, nrounds = 5,\n                     \n                     verbose = 1, maximize = T, nthread = 2)\n    gc()\n    \n    \n    xgb.save(clf, paste0(\"D:/kaggle/walmart_seg/models/\", \"clf\",\"_\", 11142015, \"_100\", \".R\") )\n    \n    #scoring to be done -- issues with function scoring\n    \n  }     \n}\n\n\n\nTime_Taken <- Sys.time() - start\n\n\n# NOT USING THE SUBMISSION FUNCTION PRED FUNCTION FOR NOW 11-10-2015\n\n#submit(clf, test, \"1172015.csv\")\n\npred <- predict(clf_extra, data.matrix(test[, feature.names])) \n\npred_1 <- matrix(pred, nrow=24838548, ncol=38, byrow = T) #there are total 38 classes \n\npred.matrix = t( matrix(pred, 38, length(pred)/38) )\n\npred <-  data.frame(t(pred_1))\n\nsample <- read_csv(\"D:/kaggle/walmart_seg/Data/sample_submission.csv\") \n\ncnames <- names(sample)[2:ncol(sample)] \n\nnames(pred.matrix) <- cnames\n\nsubmission <- cbind.data.frame(VisitNumber = visit_num , pred.matrix) \n\nsubmission <- setDT(submission)\n\nsubmission <- (submission[ , lapply(.SD, mean), by = VisitNumber])\n\nwrite_csv(submission, \"D:/kaggle/walmart_seg/submission/11152015_3.csv\")\n\n\n####################################################################################################################\n\n\n#save and retrain model later\n\n\nptrain <- predict(clf, dtrain, outputmargin = T)\n\n\nsetinfo(dtrain, \"base_margin\", ptrain)\n\n\nclf_extra <- xgboost(params = param, data = dtrain, nround = 100, verbose = 1, nthread = 2, \n                     \n                     maximize = T)\n\n\nxgb.save(clf_extra, paste0(\"D:/kaggle/walmart_seg/models/\", \"clf\",\"_\", 11142015, \"_200\", \".R\") )",
    "created" : 1446838550527.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "90464251",
    "id" : "1C343E79",
    "lastKnownWriteTime" : 1447565373,
    "path" : "D:/kaggle/walmart_seg/MODEL.R",
    "project_path" : "MODEL.R",
    "properties" : {
        "tempName" : "Untitled3"
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "type" : "r_source"
}