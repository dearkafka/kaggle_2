{
    "contents" : "set.seed(02152016)\n\n# create folds------------------------------------------------------------------------------------------\n\ndataset_blend_train = matrix(0, nrow(train), 1)\n\ndataset_blend_test_j = matrix(0, nrow(test), 5)\n\ndataset_blend_test = matrix(0, nrow(test), 1)\n\n# start iteration loop---------------------------------------------------------------------------------\n\n\n  j = 1 \n\nprint(paste(\"starting xgboost iteration ; number :\", j))\n\nset.seed(2*12*2016*j)\n\nrequire(caret)\n\nskf = createFolds(response, k = 5)\n\nprint(paste(nrow(dataset_blend_test_j),ncol(dataset_blend_test_j)))\n\n# start fold loop------------------------------------------------------------------------------------\n\n### Loop over the folds\n\ni <- 0\n\nfor (sk in skf) {\n  \n  i <- i + 1\n  \n  print(paste(\"Fold\", i))\n  \n  ### Extract and fit the train/test section for each fold\n  \n  tmp_train <- unlist(skf[i])\n  \n  x_train = train[-tmp_train,]\n  \n  y_train = response[-tmp_train]\n  \n  x_test  = train[tmp_train,]\n  \n  y_test  = response[tmp_train]\n  \n  \n  feature.names <- names(train)\n  \n  dtrain<-xgb.DMatrix(data=data.matrix(x_train),label=y_train, missing = NaN)\n  \n  param <- list( objective           = \"reg:linear\",\n                 \n                 depth = 21,\n                 \n                 min_child_weight = 40,\n                 \n                 subsample = 0.71,\n                 \n                 eta = 0.01,\n                 \n                 silent = 0\n  )\n  \n  # start training------------------------------------------------------------------------------\n  \n  print(paste(\"training xgboost for iteration :\", j, \"Fold ; number :\", i))\n  \n  mod <- xgb.train(   params              = param,\n                      \n                      booster = \"gbtree\",\n                      \n                      data                = dtrain,\n                      \n                      nrounds             = 3000,\n                      \n                      verbose             = 1,\n                      \n                      maximize            = F,\n                      \n                      nthread = 4\n                      \n  )\n  \n  dataset_blend_train[tmp_train, j] <- predict(mod, data.matrix(x_test), missing = NaN)\n  \n  \n  \n  print(paste(\"predicting xgboost for test set iteration:\", j, \" ; Fold :\", i))\n  \n  dataset_blend_test_j[, i] <- predict(mod, data.matrix(test), missing = NaN)\n  \n}\n\ndataset_blend_test[, j] <- rowMeans(dataset_blend_test_j)\n\n\n\nrequire(readr)\n\nwrite_csv(data.frame(dataset_blend_train), \"D:\\\\kaggle\\\\PRUDENTIAL\\\\blend\\\\bag\\\\xgb\\\\blend_train_xgb_02152016_7.csv\")\n\nwrite_csv(data.frame(dataset_blend_test), \"D:\\\\kaggle\\\\PRUDENTIAL\\\\blend\\\\bag\\\\xgb\\\\blend_test_xgb_02152016_7.csv\")\n\n\n\nset.seed(02*16*2016)\n\n# create folds------------------------------------------------------------------------------------------\n\ndataset_blend_train = matrix(0, nrow(train), 1)\n\ndataset_blend_test_j = matrix(0, nrow(test), 5)\n\ndataset_blend_test = matrix(0, nrow(test), 1)\n\n# start iteration loop---------------------------------------------------------------------------------\n\n\n  j = 1 \n\nprint(paste(\"starting xgboost iteration ; number :\", j))\n\nset.seed(2*12*2016*j)\n\nrequire(caret)\n\nskf = createFolds(response, k = 5)\n\nprint(paste(nrow(dataset_blend_test_j),ncol(dataset_blend_test_j)))\n\n# start fold loop------------------------------------------------------------------------------------\n\n### Loop over the folds\n\ni <- 0\n\nfor (sk in skf) {\n  \n  i <- i + 1\n  \n  print(paste(\"Fold\", i))\n  \n  ### Extract and fit the train/test section for each fold\n  \n  tmp_train <- unlist(skf[i])\n  \n  x_train = train[-tmp_train,]\n  \n  y_train = response[-tmp_train]\n  \n  x_test  = train[tmp_train,]\n  \n  y_test  = response[tmp_train]\n  \n  \n  feature.names <- names(train)\n  \n  dtrain<-xgb.DMatrix(data=data.matrix(x_train),label=y_train, missing = NaN)\n  \n  param <- list( objective           = \"reg:linear\",\n                 \n                 depth = 21,\n                 \n                 min_child_weight = 40,\n                 \n                 subsample = 0.71,\n                 \n                 eta = 0.01,\n                 \n                 silent = 0\n  )\n  \n  # start training------------------------------------------------------------------------------\n  \n  print(paste(\"training xgboost for iteration :\", j, \"Fold ; number :\", i))\n  \n  mod <- xgb.train(   params              = param,\n                      \n                      booster = \"gbtree\",\n                      \n                      data                = dtrain,\n                      \n                      nrounds             = 3000,\n                      \n                      verbose             = 1,\n                      \n                      maximize            = F,\n                      \n                      nthread = 4\n                      \n  )\n  \n  dataset_blend_train[tmp_train, j] <- predict(mod, data.matrix(x_test), missing = NaN)\n  \n  \n  \n  print(paste(\"predicting xgboost for test set iteration:\", j, \" ; Fold :\", i))\n  \n  dataset_blend_test_j[, i] <- predict(mod, data.matrix(test), missing = NaN)\n  \n}\n\ndataset_blend_test[, j] <- rowMeans(dataset_blend_test_j)\n\n\n\nrequire(readr)\n\nwrite_csv(data.frame(dataset_blend_train), \"D:\\\\kaggle\\\\PRUDENTIAL\\\\blend\\\\bag\\\\xgb\\\\blend_train_xgb_02152016_8.csv\")\n\nwrite_csv(data.frame(dataset_blend_test), \"D:\\\\kaggle\\\\PRUDENTIAL\\\\blend\\\\bag\\\\xgb\\\\blend_test_xgb_02152016_8.csv\")\n\n",
    "created" : 1455490822826.000,
    "dirty" : true,
    "encoding" : "",
    "folds" : "",
    "hash" : "1176804845",
    "id" : "1F942DA4",
    "lastKnownWriteTime" : 7011605692497750387,
    "path" : null,
    "project_path" : null,
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 15,
    "source_on_save" : false,
    "type" : "r_source"
}