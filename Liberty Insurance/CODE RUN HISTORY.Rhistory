require(Matrix)
require(caret)
require(caretEnsemble)
print(" Building xgbTree caret model")
# read in data
train <- read_csv("~/Kaggle/Liberty Insurance/CSV/train.csv")
test <- read_csv("~/Kaggle/Liberty Insurance/CSV/test.csv")
# keep copy of ID variables for test and train data
train_Id <- train$Id
test_Id <- test$Id
# response variable from training data
train_y <- train$Hazard
# predictor variables from training
train_x <- subset(train, select = -c(Id, Hazard))
train_x <- sparse.model.matrix(~., data = train_x)
# predictor variables from test
test_x <- subset(test, select = -c(Id))
test_x <- sparse.model.matrix(~., data = test_x)
#xgtrain = xgb.DMatrix(data = sparse.model.matrix(~., data = predictor),label = response)
#xgtrain <- xgb.DMatrix(data = train_xx , label= train_y)
#xgval = xgb.DMatrix(data = sparse.model.matrix(~., data = valPredictor),label = valResponse)
#CODE ADJUSTMENTS TO RUN IN PARALLEL ,CARET
NormalizedGini <- function(data, lev = NULL, model = NULL) {
SumModelGini <- function(solution, submission) {
df = data.frame(solution = solution, submission = submission)
df <- df[order(df$submission, decreasing = TRUE),]
df$random = (1:nrow(df))/nrow(df)
totalPos <- sum(df$solution)
df$cumPosFound <- cumsum(df$solution) # this will store the cumulative number of positive examples found (used for computing "Model Lorentz")
df$Lorentz <- df$cumPosFound / totalPos # this will store the cumulative proportion of positive examples found ("Model Lorentz")
df$Gini <- df$Lorentz - df$random # will store Lorentz minus random
return(sum(df$Gini))
}
solution=data$obs
submission=data$pred
result=SumModelGini(solution, submission) / SumModelGini(solution, solution)
names(result) <- "Gini"
result
}
trainControlxgbTree <- trainControl(method = "repeatedcv",number = 5,
repeats = 1,summaryFunction = NormalizedGini ,
verboseIter = T
)
xgbTree_caret <- train(x = train_x,
y = train_y,
method = "xgbTree",
trControl = trainControlxgbTree,
metric = "Gini",
"eta" = c(0.05,0.1, 0.075, 0.125),
"max_depth" = c(5, 10, 30),
'nrounds' = c(1000, 2000), savePredictions=TRUE
)
require(readr)
require(Matrix)
require(caret)
require(caretEnsemble)
print(" Building xgbTree caret model")
# read in data
train <- read_csv("~/Kaggle/Liberty Insurance/CSV/train.csv")
test <- read_csv("~/Kaggle/Liberty Insurance/CSV/test.csv")
# keep copy of ID variables for test and train data
train_Id <- train$Id
test_Id <- test$Id
# response variable from training data
train_y <- train$Hazard
# predictor variables from training
train_x <- subset(train, select = -c(Id, Hazard))
train_x <- sparse.model.matrix(~., data = train_x)
# predictor variables from test
test_x <- subset(test, select = -c(Id))
test_x <- sparse.model.matrix(~., data = test_x)
#xgtrain = xgb.DMatrix(data = sparse.model.matrix(~., data = predictor),label = response)
#xgtrain <- xgb.DMatrix(data = train_xx , label= train_y)
#xgval = xgb.DMatrix(data = sparse.model.matrix(~., data = valPredictor),label = valResponse)
#CODE ADJUSTMENTS TO RUN IN PARALLEL ,CARET
NormalizedGini <- function(data, lev = NULL, model = NULL) {
SumModelGini <- function(solution, submission) {
df = data.frame(solution = solution, submission = submission)
df <- df[order(df$submission, decreasing = TRUE),]
df$random = (1:nrow(df))/nrow(df)
totalPos <- sum(df$solution)
df$cumPosFound <- cumsum(df$solution) # this will store the cumulative number of positive examples found (used for computing "Model Lorentz")
df$Lorentz <- df$cumPosFound / totalPos # this will store the cumulative proportion of positive examples found ("Model Lorentz")
df$Gini <- df$Lorentz - df$random # will store Lorentz minus random
return(sum(df$Gini))
}
solution=data$obs
submission=data$pred
result=SumModelGini(solution, submission) / SumModelGini(solution, solution)
names(result) <- "Gini"
result
}
grid <- expand.grid(eta = c(0.05,0.1, 0.075, 0.125),
max_depth = c(5, 10, 30),
nrounds = c(1000, 2000))
trainControlxgbTree <- trainControl(method = "repeatedcv", number = 5,
repeats = 1,summaryFunction = NormalizedGini ,
verboseIter = T
)
xgbTree_caret <- train(x = train_x,
y = train_y,
method = "xgbTree" ,
trControl = trainControlxgbTree,
metric = "Gini" ,
tuneGrid = grid,
savePredictions=TRUE
)
xgbTree_caret
grid <- expand.grid(eta = c(0.05,0.04, 0.03, 0.025 ),
max_depth = c(5, 6,7),
nrounds = c(1000, 500))
trainControlxgbTree <- trainControl(method = "repeatedcv", number = 5,
repeats = 1,summaryFunction = NormalizedGini ,
verboseIter = T
)
xgbTree_caret <- train(x = train_x,
y = train_y,
method = "xgbTree" ,
trControl = trainControlxgbTree,
metric = "Gini" ,
tuneGrid = grid,
savePredictions=TRUE
)
xgbTree_caret
max(xgbTree_caret)
plot(xgbTree_caret)
save(xgbTree_caret, file = "xgbTree_caret.RData15072015")
xgbTree_caret$bestTune
xgbTree_caret$results
xgbTree_caret$modelInfo
xgbTree_caret$modelType
xgbTree_caret$results
xgbTree_caret$call
xgbTree_caret$dots
xgbTree_caret$metric
xgbTree_caret$control
xgbTree_caret$finalModel
xgbTree_caret$resample
xgbTree_caret$times
9602.82/60
xgbTree_caret$bestTune
xgbTree_caret
xgbTree_caret$results
tune_first <- xgbTree_caret$results
View(tune_first)
max(tune_first)
max(tune_first$Gini)
which.max(tune_first$Gini)
(tune_first$Gini[3])
(tune_first$Gini[3,])
tune_first[3,]
View(tune_first)
subset(tune_first, tune_first$Gini > 0.37 )
require(readr)
require(Matrix)
require(caret)
require(caretEnsemble)
print(" Building xgbTree caret model")
# read in data
train <- read_csv("~/Kaggle/Liberty Insurance/CSV/train.csv")
test <- read_csv("~/Kaggle/Liberty Insurance/CSV/test.csv")
# keep copy of ID variables for test and train data
train_Id <- train$Id
test_Id <- test$Id
# response variable from training data
train_y <- train$Hazard
# predictor variables from training
train_x <- subset(train, select = -c(Id, Hazard))
train_x <- sparse.model.matrix(~., data = train_x)
# predictor variables from test
test_x <- subset(test, select = -c(Id))
test_x <- sparse.model.matrix(~., data = test_x)
#xgtrain = xgb.DMatrix(data = sparse.model.matrix(~., data = predictor),label = response)
#xgtrain <- xgb.DMatrix(data = train_xx , label= train_y)
#xgval = xgb.DMatrix(data = sparse.model.matrix(~., data = valPredictor),label = valResponse)
#CODE ADJUSTMENTS TO RUN IN PARALLEL ,CARET
NormalizedGini <- function(data, lev = NULL, model = NULL) {
SumModelGini <- function(solution, submission) {
df = data.frame(solution = solution, submission = submission)
df <- df[order(df$submission, decreasing = TRUE),]
df$random = (1:nrow(df))/nrow(df)
totalPos <- sum(df$solution)
df$cumPosFound <- cumsum(df$solution) # this will store the cumulative number of positive examples found (used for computing "Model Lorentz")
df$Lorentz <- df$cumPosFound / totalPos # this will store the cumulative proportion of positive examples found ("Model Lorentz")
df$Gini <- df$Lorentz - df$random # will store Lorentz minus random
return(sum(df$Gini))
}
solution=data$obs
submission=data$pred
result=SumModelGini(solution, submission) / SumModelGini(solution, solution)
names(result) <- "Gini"
result
}
grid <- expand.grid(eta = c(0.04, 0.045, 0.035 ),
max_depth = c(5, 4,3),
nrounds = c(400, 500, 300, 200))
trainControlxgbTree <- trainControl(method = "repeatedcv", number = 5,
repeats = 1,summaryFunction = NormalizedGini ,
verboseIter = T
)
xgbTree_caret_first <- train(x = train_x,
y = train_y,
method = "xgbTree" ,
trControl = trainControlxgbTree,
metric = "Gini" ,
tuneGrid = grid,
savePredictions=TRUE
)
xgbTree_caret_first
xgbTree_caret_first$results
tune_second <- xgbTree_caret_first$results
write.csv(tune_first, "tune_first.csv")
write.csv(tune_second, "tune_second.csv")
require(readr)
require(Matrix)
require(caret)
require(caretEnsemble)
print(" Building xgbTree caret model")
# read in data
train <- read_csv("~/Kaggle/Liberty Insurance/CSV/train.csv")
test <- read_csv("~/Kaggle/Liberty Insurance/CSV/test.csv")
# keep copy of ID variables for test and train data
train_Id <- train$Id
test_Id <- test$Id
# response variable from training data
train_y <- train$Hazard
# predictor variables from training
train_x <- subset(train, select = -c(Id, Hazard))
train_x <- sparse.model.matrix(~., data = train_x)
# predictor variables from test
test_x <- subset(test, select = -c(Id))
test_x <- sparse.model.matrix(~., data = test_x)
#xgtrain = xgb.DMatrix(data = sparse.model.matrix(~., data = predictor),label = response)
#xgtrain <- xgb.DMatrix(data = train_xx , label= train_y)
#xgval = xgb.DMatrix(data = sparse.model.matrix(~., data = valPredictor),label = valResponse)
#CODE ADJUSTMENTS TO RUN IN PARALLEL ,CARET
NormalizedGini <- function(data, lev = NULL, model = NULL) {
SumModelGini <- function(solution, submission) {
df = data.frame(solution = solution, submission = submission)
df <- df[order(df$submission, decreasing = TRUE),]
df$random = (1:nrow(df))/nrow(df)
totalPos <- sum(df$solution)
df$cumPosFound <- cumsum(df$solution) # this will store the cumulative number of positive examples found (used for computing "Model Lorentz")
df$Lorentz <- df$cumPosFound / totalPos # this will store the cumulative proportion of positive examples found ("Model Lorentz")
df$Gini <- df$Lorentz - df$random # will store Lorentz minus random
return(sum(df$Gini))
}
solution=data$obs
submission=data$pred
result=SumModelGini(solution, submission) / SumModelGini(solution, solution)
names(result) <- "Gini"
result
}
grid <- expand.grid(eta = c(0.04, 0.045, 0.035 ),
max_depth = c(5, 6),
nrounds = c(400, 500, 600))
trainControlxgbTree <- trainControl(method = "repeatedcv", number = 5,
repeats = 1,summaryFunction = NormalizedGini ,
verboseIter = T
)
xgbTree_caret_first_1 <- train(x = train_x,
y = train_y,
method = "xgbTree" ,
trControl = trainControlxgbTree,
metric = "Gini" ,
tuneGrid = grid,
savePredictions=TRUE,
"min_child_weight" = 10,
"subsample" = .8,
"colsample_bytree" = .8,
"scale_pos_weight" = 1.0
)
xgbTree_caret_first_1
which.max(xgbTree_caret_first_1$results)
tune_third <- xgbTree_caret_first_1$results
write.csv(tune_third, "tune_third.csv")
save(xgbTree_caret_first_1, file = "xgbTree_caret.RData15072015_1")
xgbTree_caret_first_1$bestTune
require(readr)
require(Matrix)
require(caret)
require(caretEnsemble)
print(" Building xgbTree caret model")
# read in data
train <- read_csv("~/Kaggle/Liberty Insurance/CSV/train.csv")
test <- read_csv("~/Kaggle/Liberty Insurance/CSV/test.csv")
# keep copy of ID variables for test and train data
train_Id <- train$Id
test_Id <- test$Id
# response variable from training data
train_y <- train$Hazard
# predictor variables from training
train_x <- subset(train, select = -c(Id, Hazard))
train_x <- sparse.model.matrix(~., data = train_x)
# predictor variables from test
test_x <- subset(test, select = -c(Id))
test_x <- sparse.model.matrix(~., data = test_x)
#xgtrain = xgb.DMatrix(data = sparse.model.matrix(~., data = predictor),label = response)
#xgtrain <- xgb.DMatrix(data = train_xx , label= train_y)
#xgval = xgb.DMatrix(data = sparse.model.matrix(~., data = valPredictor),label = valResponse)
#CODE ADJUSTMENTS TO RUN IN PARALLEL ,CARET
NormalizedGini <- function(data, lev = NULL, model = NULL) {
SumModelGini <- function(solution, submission) {
df = data.frame(solution = solution, submission = submission)
df <- df[order(df$submission, decreasing = TRUE),]
df$random = (1:nrow(df))/nrow(df)
totalPos <- sum(df$solution)
df$cumPosFound <- cumsum(df$solution) # this will store the cumulative number of positive examples found (used for computing "Model Lorentz")
df$Lorentz <- df$cumPosFound / totalPos # this will store the cumulative proportion of positive examples found ("Model Lorentz")
df$Gini <- df$Lorentz - df$random # will store Lorentz minus random
return(sum(df$Gini))
}
solution=data$obs
submission=data$pred
result=SumModelGini(solution, submission) / SumModelGini(solution, solution)
names(result) <- "Gini"
result
}
grid <- expand.grid(eta = c(0.04, 0.045, 0.035 ),
max_depth = c(5, 6),
nrounds = c(400, 500, 600))
trainControlxgbTree <- trainControl(method = "repeatedcv", number = 5,
repeats = 1,summaryFunction = NormalizedGini ,
verboseIter = T
)
xgbTree_caret_first_2 <- train(x = train_x,
y = train_y,
method = "xgbTree" ,
trControl = trainControlxgbTree,
metric = "Gini" ,
tuneGrid = grid,
savePredictions=TRUE,
"min_child_weight" = 15,
"subsample" = .9,
"colsample_bytree" = .9,
"scale_pos_weight" = 1.2
)
save(xgbTree_caret_first_2, file = "xgbTree_caret.RData15072015_1")
xgbTree_caret_first_2
tune_fourth <- xgbTree_caret_first_2$results
write.csv(tune_fourth, "tune_fourth.csv")
xgbTree_caret
xgbTree_caret_first
xgbTree_caret_first
save(xgbTree_caret_first, file = "TUNE_SECOND.RData")
save(xgbTree_caret_first_1, file = "TUNE_THIRD.RData")
save(xgbTree_caret_first_2, file = "TUNE_FOURTH.RData")
xgbTree_caret$method
xgbTree_caret$modelInfo
xgbTree_caret$modelType
xgbTree_caret$results
xgbTree_caret$pred
xgbTree_caret$bestTune
xgbTree_caret$call
xgbTree_caret$dots
xgbTree_caret$metric
xgbTree_caret$control
xgbTree_caret$finalModel
xgbTree_caret$resample
xgbTree_caret$resampledCM
xgbTree_caret$perfNames
xgbTree_caret$maximize
xgbTree_caret$yLimits
xgbTree_caret$times
xgbTree_caret$bestTune
xgbTree_caret$call
xgbTree_caret_first$bestTune
xgbTree_caret_first_1$bestTune
xgbTree_caret_first_2$bestTune
print(" Building xgbTree caret model")
# read in data
train <- read_csv("~/Kaggle/Liberty Insurance/CSV/train.csv")
test <- read_csv("~/Kaggle/Liberty Insurance/CSV/test.csv")
# keep copy of ID variables for test and train data
train_Id <- train$Id
test_Id <- test$Id
# response variable from training data
train_y <- train$Hazard
# predictor variables from training
train_x <- subset(train, select = -c(Id, Hazard))
train_x <- sparse.model.matrix(~., data = train_x)
# predictor variables from test
test_x <- subset(test, select = -c(Id))
test_x <- sparse.model.matrix(~., data = test_x)
#xgtrain = xgb.DMatrix(data = sparse.model.matrix(~., data = predictor),label = response)
#xgtrain <- xgb.DMatrix(data = train_xx , label= train_y)
#xgval = xgb.DMatrix(data = sparse.model.matrix(~., data = valPredictor),label = valResponse)
#CODE ADJUSTMENTS TO RUN IN PARALLEL ,CARET
NormalizedGini <- function(data, lev = NULL, model = NULL) {
SumModelGini <- function(solution, submission) {
df = data.frame(solution = solution, submission = submission)
df <- df[order(df$submission, decreasing = TRUE),]
df$random = (1:nrow(df))/nrow(df)
totalPos <- sum(df$solution)
df$cumPosFound <- cumsum(df$solution) # this will store the cumulative number of positive examples found (used for computing "Model Lorentz")
df$Lorentz <- df$cumPosFound / totalPos # this will store the cumulative proportion of positive examples found ("Model Lorentz")
df$Gini <- df$Lorentz - df$random # will store Lorentz minus random
return(sum(df$Gini))
}
solution=data$obs
submission=data$pred
result=SumModelGini(solution, submission) / SumModelGini(solution, solution)
names(result) <- "Gini"
result
}
grid <- expand.grid(eta = c(0.04, 0.045, 0.035 ),
max_depth = c(5, 6),
nrounds = c(400, 500, 600))
trainControlxgbTree <- trainControl(method = "repeatedcv", number = 5,
repeats = 1,summaryFunction = NormalizedGini ,
verboseIter = T
)
xgbTree_caret_first_3 <- train(x = train_x,
y = train_y,
method = "xgbTree" ,
trControl = trainControlxgbTree,
metric = "Gini" ,
tuneGrid = grid,
savePredictions=TRUE,
"min_child_weight" = 20,
"subsample" = .7,
"colsample_bytree" = .8,
"scale_pos_weight" = 1.5
)
xgbTree_caret_first_3
save(xgbTree_caret_first_3, file = "TRAIN_FIFTH.RData")
tune_fifth <- xgbTree_caret_first_3$results
write.csv(tune_fifth, 'tune_fifth.csv')
savehistory("~/Kaggle/Liberty Insurance/CODE RUN HISTORY.Rhistory")
print(" Building xgbTree caret model")
# read in data
train <- read_csv("~/Kaggle/Liberty Insurance/CSV/train.csv")
test <- read_csv("~/Kaggle/Liberty Insurance/CSV/test.csv")
# keep copy of ID variables for test and train data
train_Id <- train$Id
test_Id <- test$Id
# response variable from training data
train_y <- train$Hazard
# predictor variables from training
train_x <- subset(train, select = -c(Id, Hazard))
train_x <- sparse.model.matrix(~., data = train_x)
# predictor variables from test
test_x <- subset(test, select = -c(Id))
test_x <- sparse.model.matrix(~., data = test_x)
#xgtrain = xgb.DMatrix(data = sparse.model.matrix(~., data = predictor),label = response)
#xgtrain <- xgb.DMatrix(data = train_xx , label= train_y)
#xgval = xgb.DMatrix(data = sparse.model.matrix(~., data = valPredictor),label = valResponse)
#CODE ADJUSTMENTS TO RUN IN PARALLEL ,CARET
NormalizedGini <- function(data, lev = NULL, model = NULL) {
SumModelGini <- function(solution, submission) {
df = data.frame(solution = solution, submission = submission)
df <- df[order(df$submission, decreasing = TRUE),]
df$random = (1:nrow(df))/nrow(df)
totalPos <- sum(df$solution)
df$cumPosFound <- cumsum(df$solution) # this will store the cumulative number of positive examples found (used for computing "Model Lorentz")
df$Lorentz <- df$cumPosFound / totalPos # this will store the cumulative proportion of positive examples found ("Model Lorentz")
df$Gini <- df$Lorentz - df$random # will store Lorentz minus random
return(sum(df$Gini))
}
solution=data$obs
submission=data$pred
result=SumModelGini(solution, submission) / SumModelGini(solution, solution)
names(result) <- "Gini"
result
}
grid <- expand.grid(eta = c(0.04, 0.035 ),
max_depth = c(5, 4),
nrounds = c(400))
trainControlxgbTree <- trainControl(method = "repeatedcv", number = 10,
repeats = 1,summaryFunction = NormalizedGini ,
verboseIter = T
)
xgbTree_caret_first_4 <- train(x = train_x,
y = train_y,
method = "xgbTree" ,
trControl = trainControlxgbTree,
metric = "Gini" ,
tuneGrid = grid,
savePredictions=TRUE,
"min_child_weight" = 40,
"subsample" = .5,
"colsample_bytree" = .5,
"scale_pos_weight" = 3.5
)
tune_six <- xgbTree_caret_first_4$results
write.csv(tune_six, 'tune_six.csv')
save(xgbTree_caret_first_4, file = "TRAIN_SIX.RData")
xgbTree_caret_first_4
xgbTree_caret_first_3
pred <- predict(xgbTree_caret_first_3, data=test_x)
pred <- predict(xgbTree_caret_first_3, newdata=test_x)
pred
sub <- data.frame("Id" = test_Id, "Hazard" = pred)
write.csv(sub, "fifth.csv")
View(sub)
sub <- data.frame("Id" = test_Id, "Hazard" = pred, row.names = F)
pred <- predict(xgbTree_caret_first_3, newdata=test_x)
sub <- data.frame("Id" = test_Id, "Hazard" = pred, row.names = F)
sub <- data.frame("Id" = test_Id, "Hazard" = pred, row.names = FALSE)
sub <- data.frame("Id" = test_Id, "Hazard" = pred)
write.csv(sub, "fifth.csv", row.names = FALSE)
pred <- predict(xgbTree_caret_first_2, newdata=test_x)
sub <- data.frame("Id" = test_Id, "Hazard" = pred)
write.csv(sub, "sixth.csv", row.names = FALSE)
pred <- predict(xgbTree_caret_first_1, newdata=test_x)
sub <- data.frame("Id" = test_Id, "Hazard" = pred)
write.csv(sub, "eighth.csv", row.names = FALSE)
# submission predict
pred <- predict(xgbTree_caret_first, newdata=test_x)
sub <- data.frame("Id" = test_Id, "Hazard" = pred)
write.csv(sub, "seventh.csv", row.names = FALSE)
savehistory("~/Kaggle/Liberty Insurance/CODE RUN HISTORY.Rhistory")
