{
    "contents" : "\n# combine bag_xgb, bag_rf, bag_gbm, bag_glm\n\n# use dataset_blend_train to train a second level model-------------------------------------------\n\ntrain <- cbind(train, data.frame(dataset_blend_train))\n\ntest <- cbind(test, data.frame(dataset_blend_test))\n\n\n# train a second level using glm(log_reg)-------------------------------------------------------------\n\ntarget <- as.factor(response)\n\ntrain$target <- target\n\nmyX <- names(train)\n\nmyY <- \"target\"\n\nprint(paste(\"training glm iteration :\", j, \"for Fold ; number :\", i))\n\ntrain.hex <- as.h2o(train)\n\ntest_glm <- h2o.glm( x = myX,\n                     \n                     y = myY,\n                     \n                     training_frame = train.hex,\n                     \n                     family = \"binomial\",\n                     \n                     lambda_search = TRUE,\n                     \n                     link = \"logit\", \n                     \n                     standardize = T\n                     )\n\ntest.hex <- as.h2o(test)\n\npred_glm <- h2o.predict(object = test_glm, newdata = test.hex)\n\npred_glm <- as.data.frame(pred_glm)\n\npreds <- pred_glm$p1\n\n## create submission file-----------------------------------------------------------------------\n\nsubmission = data.frame(Id = id)\n\nsubmission$Response = as.integer(preds)\n\nwrite_csv(submission, \"D:\\\\kaggle\\\\PRUDENTIAL\\\\submission\\\\02062016_glm.csv\")\n\n\n#############################################################################################\n#############################################################################################\n\n# run xgboost--------------------------------------------------------------------------------\n\n# independant of glm------------------------------------------------------------------------\n\ndataset_blend_train <- read_csv(\"D:\\\\kaggle\\\\HOMESITE\\\\blend\\\\bag\\\\FINAL\\\\blend_train_02052016.csv\")\n\ndataset_blend_test <- read_csv(\"D:\\\\kaggle\\\\HOMESITE\\\\blend\\\\bag\\\\FINAL\\\\blend_test_02052016.csv\")\n\ntrain <- cbind(train, data.frame(dataset_blend_train))\n\ntest <- cbind(test, data.frame(dataset_blend_test))\n\nset.seed(2*07*2016)\n\nfeature.names <- names(train)\n\ndtrain<-xgb.DMatrix(data=data.matrix(train),label=response, missing = NaN)\n\nparam <- list(  objective           = \"binary:logistic\", \n                \n                booster = \"gbtree\",\n                \n                eval_metric = \"auc\",\n                \n                eta                 = 0.023, # 0.06, #0.01,\n                \n                max_depth           = 6, #changed from default of 8\n                \n                subsample           = 0.83, # 0.7\n                \n                colsample_bytree    = 0.77, # 0.7\n                \n                num_parallel_tree = 2\n                \n)\n\n\nmod <- xgb.train(   params              = param,\n                    \n                    data                = dtrain,\n                    \n                    nrounds             = 3000,\n                    \n                    verbose             = 1,\n                    \n                    maximize            = T,\n                    \n                    nthread = 4)\n\n\npred <- predict(mod, data.matrix(test), missing = NaN)\n\n\n## create submission file-----------------------------------------------------------------------\n\nsubmission <- data.frame(QuoteNumber = id, QuoteConversion_Flag = pred)\n\nwrite_csv(submission, \"D:\\\\kaggle\\\\HOMESITE\\\\submission\\\\02072016_1_xgb.csv\")\n",
    "created" : 1454660972347.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2784620792",
    "id" : "DCAF7487",
    "lastKnownWriteTime" : 1454835531,
    "path" : "D:/kaggle/HOMESITE/blend/blend_model_glm.R",
    "project_path" : "blend/blend_model_glm.R",
    "properties" : {
    },
    "relative_order" : 7,
    "source_on_save" : false,
    "type" : "r_source"
}